# è‡ªç„¶è¯­è¨€å¤„ç† é˜…è¯»æŠ¥å‘Šï¼ˆAttention Is All You Needï¼‰

> groupï¼šé¾™æµ©æ³‰ã€æå®‡å“²ã€æä¹çª
>
> å°ç»„åˆä½œåŠåˆ†å·¥ï¼š
>
> - é¾™æµ©æ³‰ï¼šå®éªŒæŠ¥å‘Šå…­ã€ä¸ƒéƒ¨åˆ†ï¼Œä»¥åŠï¼šè¿œç¨‹SSHæœåŠ¡å™¨çš„ç¯å¢ƒé…ç½®ï¼Œä»£ç çš„åˆ†æç†è§£ã€è¿è¡Œä¸ç»“æœå¤ç°ï¼Œæ–¹æ³•ä¼˜åŒ–ï¼ˆVITï¼‰çš„å®ç°ä¸å¯¹æ¯”
> - æå®‡å“²ï¼šå®éªŒæŠ¥å‘Š ä¸€ã€äºŒã€ä¸‰éƒ¨åˆ†ï¼Œä»£ç çš„åˆ†æç†è§£
> - æä¹çªï¼šå®éªŒæŠ¥å‘Šå››ã€äº”éƒ¨åˆ†ï¼Œä»£ç çš„åˆ†æç†è§£ã€è¿è¡Œä¸å¤ç°



## ä¸€ã€èƒŒæ™¯ä»‹ç»

### 1.1 è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯

#### 1.1.1 è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜

è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ 

> ç”¨è®¡ç®—æœºå¯¹è‡ªç„¶è¯­è¨€çš„å½¢ã€éŸ³ã€ä¹‰ç­‰ä¿¡æ¯è¿›è¡Œå¤„ç†ã€‚å³ä¸å­—ã€è¯ã€å¥ã€ç¯‡ç« çš„è¾“å…¥ã€è¾“å‡ºã€è¯†åˆ«ã€åˆ†æã€ç†è§£ã€ç”Ÿæˆç­‰çš„æ“ä½œå’ŒåŠ å·¥

å¦‚ä½•å»ºæ¨¡è¯­è¨€ï¼Ÿ

>äººç±»è¯­è¨€æ˜¯ä¸€ç§ä¸Šä¸‹æ–‡ç›¸å…³çš„ä¿¡æ¯è¡¨è¾¾æ–¹å¼ï¼Œå› æ­¤è¦è®©æœºå™¨èƒ½å¤Ÿå¤„ç†è‡ªç„¶æºï¼Œå°±è¦å…ˆä¸ºå…¶å»ºç«‹æ•°å­¦æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹è¢«ç§°ä¸º `ç»Ÿè®¡è¯­è¨€æ¨¡å‹`
>
>- å³åˆ¤æ–­ä¸€ä¸ªæ–‡å­—åºåˆ—æ˜¯å¦èƒ½å¤Ÿæ„æˆäººç±»èƒ½ç†è§£å¹¶ä¸”æœ‰æ„ä¹‰çš„å¥å­

å¯¹äºè¯­è¨€è¿™ç§åºåˆ—æ¨¡å‹ï¼Œæ¯ä¸€ä¸ªè¾“å…¥$x_t$ä¸å…¶å‰åºçš„è¾“å…¥ $x_1,...x_{t-1}$æœ‰å…³ï¼Œå› æ­¤ç›´æ¥ç”¨mlpæˆ–è€…CNNè¿™æ ·çš„è®¤ä¸ºæ¯ä¸ªè¾“å…¥ä¹‹é—´æ˜¯ç‹¬ç«‹çš„æ¨¡å‹ä¼šå¤±å»è¾“å…¥ä¹‹é—´çš„è”ç³»ã€‚

å‡è®¾åœ¨å•è¯çº§åˆ«å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œè¯å…ƒåŒ–
$$
P(x_1,...,x_T) = \Pi_{t=1}^{T}P(x_t|x_1,...,x_{t-1})
$$


- ä»»æ„ä¸€ä¸ªè¾“å…¥è¯è¯­$x_i$å‡ºç°çš„æ¦‚ç‡éƒ½å–å†³äºå®ƒå‰é¢å‡ºç°çš„æ‰€æœ‰è¯
- ä½†éšç€æ–‡æœ¬é•¿åº¦çš„å¢åŠ ï¼Œ$P(x_t|x_1,...,x_{t-1})$ä¸å¤ªå¤šå‰åºè¾“å…¥æœ‰å…³ï¼Œå¾ˆéš¾è®¡ç®—ï¼Œå› æ­¤å®é™…è®¡ç®—é•¿å‡è®¾æ¯æ¬¡è¾“å…¥è¯å…ƒçš„æ¦‚ç‡ä»…å’Œå…¶å‰åºNä¸ªè¯å…ƒæœ‰å…³

$$
P(x_i|x_1,x_2,...,x_{i-1}) = P(x_i|x_{i-N+1},..,x_{i-1})
$$

è¿™ç§å‡è®¾å³**markovå‡è®¾**ï¼Œå¯¹åº”çš„è¯­è¨€æ¨¡å‹ç§°ä¸º**N-gramæ¨¡å‹**

é«˜é˜¶çš„è¯­è¨€æ¨¡å‹éœ€è¦ç”¨RNNã€LSTMã€GRUã€Transfomerç­‰æ¨¡å‹å»æ•æ‰è¯è¯­ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–æ€§ã€‚



#### 1.1.2 è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯åˆ†ç±»

è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å¯åˆ†ä¸º

- åŸºç¡€æŠ€æœ¯
- åº”ç”¨æŠ€æœ¯

å…¶ä¸­åŸºç¡€æŠ€æœ¯åŒ…æ‹¬ï¼š

- è¯æ³•ä¸å¥æ³•åˆ†æã€è¯­ä¹‰åˆ†æã€è¯­ç”¨åˆ†æã€ç¯‡ç« åˆ†æç­‰

åº”ç”¨æŠ€æœ¯åŒ…æ‹¬ï¼š

- æœºå™¨ç¿»è¯‘ã€ä¿¡æ¯æ£€ç´¢ã€æƒ…æ„Ÿåˆ†æã€è‡ªåŠ¨é—®ç­”ã€è‡ªåŠ¨æ–‡æ‘˜ã€ä¿¡æ¯æŠ½å–ã€ä¿¡æ¯æ¨èä¸è¿‡æ»¤ã€æ–‡æœ¬åˆ†ç±»ä¸èšç±»ã€æ–‡å­—è¯†åˆ«ç­‰

**è€Œtransformeræœ€åˆæå‡ºæ˜¯åŸºäºnlpä¸­çš„æœºå™¨ç¿»è¯‘é—®é¢˜**ï¼Œä½†transformeræ¶æ„åç»­åœ¨è®¸å¤šå…¶ä»–é¢†åŸŸä¹Ÿæœ‰æ˜¾è‘—åº”ç”¨ï¼Œä½œè€…åœ¨conclusionä¸­é¢„æµ‹åˆ°äº†è¿™ä¸€ç‚¹ã€‚



### 1.2 æœºå™¨ç¿»è¯‘

**æœºå™¨ç¿»è¯‘ï¼ˆMachine Translationï¼‰**

> é€šè¿‡ç‰¹å®šçš„è®¡ç®—æœºç¨‹åºå°†ä¸€ç§ä¹¦å†™å½¢å¼æˆ–å£°éŸ³å½¢å¼çš„è‡ªç„¶è¯­è¨€ï¼Œç¿»è¯‘æˆå¦ä¸€ç§ä¹¦å†™å½¢å¼æˆ– å£°éŸ³å½¢å¼çš„è‡ªç„¶è¯­è¨€

* æ–¹æ³•ï¼š

  - åŸºäºç†æ€§çš„ç ”ç©¶æ–¹æ³•â€”â€”åŸºäºè§„åˆ™çš„æ–¹æ³•

  - åŸºäºç»éªŒçš„ç ”ç©¶æ–¹æ³•â€”â€”åŸºäºç»Ÿè®¡çš„æ–¹æ³•

  - ä¸æ·±åº¦å­¦ä¹ ç›¸ç»“åˆ


è€Œtransformerå±äºä¸æ·±åº¦å­¦ä¹ ç›¸ç»“åˆï¼ŒåŸºäºç¥ç»ç½‘ç»œï¼Œä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå¹¶å°†åŸå…ˆä¸»æµçš„RNNï¼ŒLSTMç­‰layeræ›¿æ¢æˆäº†self-attentionæœºåˆ¶ï¼Œåœ¨encoderä¸­ä½¿ç”¨äº†ä¸€ç§attentionï¼Œåœ¨decoderä¸­ä½¿ç”¨äº†ä¸¤ç§attentionï¼Œæ¥è§£å†³è‹±è¯­â€”â€”å¾·è¯­ç¿»è¯‘ï¼ˆWMT2014ï¼‰è¿™ä¸ªæ•°æ®é›†é—®é¢˜ã€‚



### 1.3 è®ºæ–‡ç ”ç©¶èƒŒæ™¯

åœ¨è®ºæ–‡ç ”ç©¶ä¹‹å‰ï¼Œä¸»æµçš„åºåˆ—è½¬æ¢æ¨¡å‹éƒ½æ˜¯åŸºäºå¤æ‚çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œä¸”éƒ½ä½¿ç”¨äº† encoder-decoderæ¶æ„ã€‚è¡¨ç°æ¯”è¾ƒå¥½çš„æ¨¡å‹æ˜¯é€šè¿‡attentionæœºåˆ¶æŠŠencoderå’Œdecoderè¿æ¥ã€‚

è€Œè¿™ç¯‡è®ºæ–‡æ˜¯åœ¨åš sequence -> sequence çš„ç”Ÿæˆï¼Œä¸”ä¹‹åŸºäºå•ç‹¬çš„attentionæœºåˆ¶ï¼Œè€Œå®Œå…¨é¿å…ä½¿ç”¨RNNæˆ–è€…CNN

- **RNN**

  - å¤„ç†åºåˆ—æ•°æ®æ—¶ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—ä¾èµ–äºå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå› æ­¤å¿…é¡»æŒ‰æ—¶é—´æ­¥é¡ºåºæ‰§è¡Œï¼Œåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„**å…³é”®è·¯å¾„**è¾ƒé•¿ï¼Œå¯¼è‡´å¹¶è¡ŒåŒ–è®­ç»ƒæ¯”è¾ƒå·®ï¼›

  - ç°æœ‰GPUç­‰ç¡¬ä»¶åœ¨å¤„ç†å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—æ¯”è¾ƒé«˜æ•ˆï¼Œä½†RNNæ˜¯é¡ºåºè®¡ç®—çš„ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç¡¬ä»¶çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼›

  - å› æ­¤å¦‚æœèƒ½å¤Ÿåœ¨encoder-decoderæ¶æ„ä¸­é¿å…ä½¿ç”¨RNNï¼Œä¼šåœ¨è®­ç»ƒæ—¶é—´ä¸Šæœ‰ä¸é”™çš„æå‡ã€‚


  >LSTMå’ŒGRUç›¸è¾ƒäºTransformerï¼Œåªæ˜¯å¼•å…¥äº†é—¨æ§æœºåˆ¶ï¼Œç¼“è§£äº†RNNçš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ï¼Œå¹¶æ²¡æœ‰è§£å†³å¹¶è¡Œæ€§å·®çš„é—®é¢˜ï¼Œè€Œtransformerè¿™ç§åŸºäºattentionçš„æœºåˆ¶ï¼Œå¯ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå…·ä½“åŸå› åœ¨åç»­ä¼šæœ‰è¯´æ˜

- **CNN**

  - CNNæ˜¯é€šè¿‡kernelï¼ˆå·ç§¯æ ¸ï¼‰æ•è·å±€éƒ¨ç‰¹å¾ï¼Œè€Œattentionæœºåˆ¶èƒ½å¤Ÿæ•æ‰åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªå…ƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œç›¸è¾ƒäºå·ç§¯å±‚ä¼šæ›´é€‚åˆåºåˆ—ï¼›
  - åŒæ ·ï¼Œself-attentionå…è®¸å¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œä½†CNNçš„å·ç§¯æ“ä½œéœ€è¦åœ¨ä¸åŒlayerä¹‹é—´ä¼ é€’ä¿¡æ¯ï¼Œå¦‚æœè¦æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œéœ€è¦å¤šå±‚layerï¼Œç”¨å¤šä¸ªpatternå»æ•æ‰ï¼Œå¹¶è¡Œæ€§äº¤å‰ï¼›
  - åŒæ—¶ï¼Œé•¿åºåˆ—é—®é¢˜ä¸­ï¼ŒCNNå¯èƒ½å‡ºç°é•¿è·ç¦»ä¾èµ–é—®é¢˜ï¼ŒåªåŸºäºattentionçš„transformerä¼šæœ‰æ›´å¥½çš„è¡¨ç°ã€‚



#### RNN

>![image-20240609163007701](./assets/image-20240609163007701.png)
>
>* å…·æœ‰éšçŠ¶æ€çš„å¾ªç¯ç¥ç»ç½‘ç»œ
>
>å‡è®¾åœ¨æ—¶é—´æ­¥t æœ‰å°æ‰¹é‡è¾“å…¥ $X_t \in R^{n \times d}$ï¼Œä¸ºä¸€ä¸ªæ ·æœ¬
>
>$H_t \in R^{n \times h}$è¡¨ç¤ºæ—¶é—´æ­¥tçš„éšè—å˜é‡ï¼ŒåŒæ—¶ä¿å­˜èµ·å“ªä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—å˜é‡ $H_{t-1}$
>
>æœ‰ä¸¤ä¸ªå¯ä»¥å­¦ä¹ çš„æƒé‡å‚æ•° ï¼š
>$$
>W_{hh} \in R^{h \times h}ï¼Œ W_{xh} \in R^{x \times h}
>$$
>éšè—å˜é‡çš„è®¡ç®— ï¼š
>$$
>H_t = \phi(X_tW_{xh} + H_{t-1}W_{hh} + b_h)
>$$
>è¾“å‡ºï¼š
>$$
>O_t = H_t W_{hq} + b_q
>$$



#### CNN

>å·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰æå–å±€éƒ¨ä¿¡æ¯ï¼Œæ± åŒ–å±‚ï¼ˆPooling Layerï¼‰åšæ±‡èšæ“ä½œ
>
>![image-20240609170658169](./assets/image-20240609170658169.png)
>
>æ¯”å¦‚è¿™æ˜¯ä¸€ä¸ªå·ç§¯æ ¸çš„æ ¸å‡½æ•°ï¼Œå¤§å°æ˜¯ $2\times 2$ï¼Œæ¯ä¸ªå·ç§¯å±‚åšäº’ç›¸å…³è¿ç®—ï¼Œç”¨äºæå–pattern
>
>CNNé€šè¿‡å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰å’Œæ± åŒ–å±‚ï¼ˆPooling Layerï¼‰æ„å»ºè€Œæˆï¼Œå…·æœ‰å¯¹å›¾åƒç­‰äºŒç»´æ•°æ®è¿›è¡Œç‰¹å¾æå–å’Œæ¨¡å¼è¯†åˆ«çš„èƒ½åŠ›



#### æ³¨æ„åŠ›æœºåˆ¶

>æ³¨æ„åŠ›æœºåˆ¶æºè‡ªå¿ƒç†å­¦ä¸­çš„è‡ªä¸»æ€§æç¤ºå’Œéè‡ªä¸»æ€§æç¤ºï¼Œæ˜ å°„åˆ°attentionä¸­ï¼Œæ¯ä¸€ä¸ªæ„å¿—çº¿ç´¢æ˜¯ä¸€ä¸ªqueryï¼ŒæŸ¥è¯¢æ‰€æœ‰çš„é”®å€¼å¯¹ï¼ˆkey-valueï¼‰
>
>valueé‡åº¦ä¸€ä¸ªqueryå’Œå½“å‰keyçš„ç›¸ä¼¼ç¨‹åº¦
>
>![image-20240609171136019](./assets/image-20240609171136019.png)
>
>transformerä¸­ç”¨åˆ°äº†ç‚¹ç§¯æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›
>
>- ç‚¹ç§¯æ³¨æ„åŠ›ï¼š
>
>  è®¡ç®—æŸ¥è¯¢å‘é‡å’Œkeyä¹‹é—´çš„ç‚¹ç§¯ï¼Œç„¶åå°†ç»“æœè¿›è¡Œå½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
>
>- è‡ªæ³¨æ„åŠ›ï¼š
>
>  è¾“å…¥å’Œè¾“å‡ºéƒ½æ¥è‡ªåŒä¸€ä¸ªæ–°åºåˆ—ï¼Œç”¨äºå»ºæ¨¡åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»



Transformeråœ¨WMT2014 è‹±è¯­-å¾·è¯­ç¿»è¯‘ä»»åŠ¡ä¸Šå»å¾—äº† 28.4çš„BLEUè¯„åˆ†ï¼Œåœ¨å½“æ—¶è¡¨ç°æœ€å¥½çš„æ¨¡å‹çš„åŸºç¡€ä¸Šæé«˜äº†2ä¸ªBELUè¯„åˆ†ï¼›

åœ¨WMT2014è‹±è¯­-å‘è‚²ç¿»è¯‘ä»»åŠ¡ä¸Šå»å¾—äº†41.8ä¸ªBLEUè¯„åˆ†ï¼›

åŸè®ºæ–‡çš„å®éªŒæ˜¯é‡‡ç”¨ 8å¼  P100 è®­ç»ƒäº†3.5å¤©ã€‚ç”±äºæˆ‘ä»¬å¹¶æ²¡æœ‰è¶³å¤Ÿçš„èµ„é‡‘ç§Ÿåˆ°ç›¸åº”çš„è®¡ç®—èµ„æºï¼Œæ‰€ä»¥æˆ‘ä»¬æ˜¯åœ¨å•å¡ï¼ˆ3090ï¼‰ä¸‹è®­ç»ƒå…¶å°æ•°æ®é›†ï¼Œè‡³äºåˆ†å¸ƒå¼è®­ç»ƒçš„æ–¹å¼å’Œç­–ç•¥ï¼Œä¼šåœ¨åç»­æœ‰æ‰€æ¢ç´¢ï¼Œä½†å¹¶æ²¡æœ‰å®é™…ä½¿ç”¨ã€‚

#### BLEU

>BELU scoreæ˜¯ä¸€ç§å¸¸ç”¨çš„æœºå™¨ç¿»è¯‘è´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æœºå™¨ç¿»è¯‘ç»“æœä¸å‚è€ƒç¿»è¯‘ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
>
>BLEUåˆ†æ•°é€šè¿‡è®¡ç®—è¯‘æ–‡å’Œå‚è€ƒç¿»è¯‘ä¹‹é—´çš„n-gramé‡åˆç¨‹åº¦æ¥è¯„ä¼°
>
>å…·ä½“è®¡ç®—æ­¥éª¤
>
>- n-gramåŒ¹é…
>
>  n-gramå·²ç»åœ¨1.1ä¸­æœ‰æ‰€è¯´æ˜ï¼ŒBLEUåˆ†æ•°é€šè¿‡è®¡ç®—æœºå™¨ç¿»è¯‘ç»“æœä¸­n-gramä¸å‚è€ƒç¿»è¯‘è¿™ç§n-gramçš„åŒ¹é…æƒ…å†µæ¥è¯„ä¼°ç¿»è¯‘è´¨é‡
>
>  æ¯”å¦‚åœ¨ä¸‹é¢è¿™ä¸ªæˆ‘ä»¬å®é™…è®­ç»ƒçš„ç»“æœä¸­
>
>  ![image-20240609161118543](./assets/image-20240609161118543.png)
>
>  1-gramå°±æ˜¯æ¯ä¸ªå•è¯å‡ºç°åœ¨å‚è€ƒç¿»è¯‘ä¸­çš„æ¬¡æ•°æ¯”å¦‚ladyæ˜¯1ï¼Œè€Œandæ˜¯0
>
>- ç²¾ç¡®åº¦ï¼ˆprecisionï¼‰
>
>  è®¡ç®—è¯‘æ–‡ä¸­åŒ¹é…çš„n-gramçš„æ•°é‡ä¸æ€»-gramæ•°é‡çš„æ¯”ç‡ã€‚ä¾‹å¦‚ï¼Œ1-gramç²¾ç¡®åº¦æ˜¯è¯‘æ–‡ä¸­åŒ¹é…çš„1-gramæ•°é‡é™¤ä»¥è¯‘æ–‡ä¸­æ€»çš„1-gramçš„æ•°é‡
>
>- é•¿åº¦æƒ©ç½šï¼ˆBrevity Penaltyï¼ŒBPï¼‰
>
>  BLEUåˆ†æ•°åŒ…å«ä¸€ä¸ªé•¿åº¦æƒ©ç½šå› å­ï¼Œä»¥é¿å…æœºå™¨ç¿»è¯‘é€šè¿‡ç”Ÿæˆæœæ–­çš„è¯‘æ–‡æ¥è·å–é«˜åˆ†
>
>  $ BP = \begin{cases}  1 & \text{if } c > r \\ e^{(1 - r/c)} & \text{if } c \leq r  \end{cases} $
>
>- BLEUåˆ†æ•°è®¡ç®—
> $$
>  BLEU = BP \cdot \exp(\sum_{n=1}â€¦â€¦{N}w_i\log p_n)
> $$
>
>  - $p_n$æ˜¯n-gramç²¾ç¡®åº¦
>  - $w_n$æ˜¯æƒé‡



### 1.4 transformer

transformerä½¿ç”¨self-attentionæœºåˆ¶ï¼Œå°†ä¸€ä¸ªåºåˆ—çš„ä¸åŒä½ç½®è”ç³»èµ·æ¥ï¼Œä»¥è®¡ç®—åºåˆ—çš„è¡¨ç¤ºã€‚

self-attentionä¼šåœ¨åç»­ *å››ã€ æ¨¡å‹æ¶æ„* ä¸€èŠ‚ä¸­è¯¦ç»†è¯´æ˜



## äºŒã€ä¼ ç»Ÿæ–¹æ³•å±€é™ä¸ç ”ç©¶æ–¹æ³•ä¼˜åŠ¿

### 2.1 è®ºæ–‡èƒŒæ™¯ä¸‹ç ”ç©¶æ–¹æ³•

æœºå™¨ç¿»è¯‘éœ€è¦è¿›è¡Œåºåˆ—å»ºæ¨¡å’Œè½¬æ¢ï¼Œå¯¹äºè¯­è¨€æ¨¡å‹å’Œæœºå™¨ç¿»è¯‘ã€‚transformeræå‡ºä¹‹å‰ï¼Œå¤šç”¨ RNNã€LSTMã€GRUç­‰æ¨¡å‹ï¼ŒåŸºäºencoder-decoderæ¶æ„è¿›è¡Œç ”ç©¶

transformeræå‡ºä¹‹å‰ï¼Œæœºå™¨ç¿»è¯‘é¢†åŸŸä¸»è¦é‡‡ç”¨

- åŸºäºç»Ÿè®¡çš„æ–¹æ³•
- åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•

æ¯”å¦‚

- ç»Ÿè®¡æœºå™¨ç¿»è¯‘ï¼ˆSMTï¼‰ï¼šåˆ©ç”¨ç»Ÿè®¡å»ºæ¨¡ï¼Œå¤§è§„æ¨¡çš„åŒè¯­å¹³è¡Œè¯­æ–™è¿›è¡Œè®­ç»ƒ
- åŸºäºçŸ­è¯­çš„æœºå™¨ç¿»è¯‘
- NMTï¼Œåœ¨ç¥ç»ç½‘ç»œæ¡†æ¶çˆ±å–œçˆ±è¿›è¡Œæœºå™¨ç¿»è¯‘çš„æ–¹æ³•ï¼ŒNMTç›´æ¥å°†æºè¯­è¨€å¥å­æ˜ å°„åˆ°ç›®æ ‡è¯­è¨€å¥å­çš„è¿‡ç¨‹ä½œä¸ºä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œå­¦äº›å’Œé¢„æµ‹
- åŸºäºè®°å¿†çš„ç¥ç»æœºå™¨ç¿»è¯‘



### 2.2 è®ºæ–‡èƒŒæ™¯ä¸‹å±€é™æ€§

åœ¨Transformeræå‡ºä¹‹å‰ï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†é•¿åºåˆ—ï¼Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œè¿˜æœ‰å¹¶è¡Œè®­ç»ƒå’Œæ—¶é—´ç­‰æ–¹é¢å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼ŒåŸå› åœ¨1.3èŠ‚å·²ç»æœ‰è¿‡é˜è¿°ã€‚



### 2.3 transformerçš„ä¼˜è¶Šæ€§

transformerå®Œå…¨ä½¿ç”¨self-attentionæœºåˆ¶è®¡ç®—å…¶è¾“å…¥å’Œè¾“å‡ºè¡¨ç¤ºï¼Œæ”¾å¼ƒäº†åŸæœ‰çš„RNN layerï¼Œä»è€Œæé«˜çš„è®¡ç®—çš„å¯å¹¶è¡ŒåŒ–

* **ä¸ºä»€ä¹ˆé€‰æ‹© self-attention**

![image-20240609210919520](./assets/image-20240609210919520.png)

è®ºæ–‡ä¸­æ¯”è¾ƒäº†å››ç§å±‚ä¸‹çš„è®¡ç®—å¤æ‚åº¦ã€é¡ºåºè®¡ç®—ï¼ˆæ¯”è¾ƒéš¾å¹¶è¡ŒåŒ–ï¼‰ã€ä¿¡æ¯ä»ä¸€ä¸ªæ•°æ®ç‚¹åˆ°å¦ä¸€ä¸ªæ•°æ®ç‚¹çš„è·ç¦»

å¯¹äºself-attentionçš„ç®—æ³•å¤æ‚åº¦æ˜¯ $o(n^2 \cdot d)$ï¼Œé¡ºåºè®¡ç®—æ˜¯$O(1)$çš„ï¼Œæ‰€ä»¥æ¯”è¾ƒå¥½å¹¶è¡ŒåŒ–ï¼Œä»»ä½•ä¸€ä¸ªqueryå’Œä¸€ä¸ªå¾ˆè¿œçš„key-value pairåªéœ€è¦ä¸€æ¬¡è®¡ç®—å°±èƒ½å°†ä¿¡æ¯ä¼ é€’è¿‡æ¥ï¼Œå› æ­¤ï¼Œmax-mum Path Lengthä¹Ÿæ˜¯ $O(1)$çš„

ç›¸è¾ƒäºRNNçš„é¡ºåºè®¡ç®—æ˜¯$O(n)$ï¼Œéš¾ä»¥å¹¶è¡ŒåŒ–ï¼Œè€Œä¸”å¯¹é•¿åºåˆ—çš„è¾“å…¥è¡¨è¾¾èƒ½åŠ›å¹¶ä¸å¥½

CNNçš„ä¿¡æ¯ä¼ é€’æ˜¯$O(log_k(n))$çš„ï¼Œä¼šæ¯”RNNå¥½ä¸€äº›

å¯ä»¥å‘ç°ï¼Œself-attentionå¯¹äºå¤„ç†é•¿åºåˆ—ï¼Œå’Œå¯å¹¶è¡ŒåŒ–ï¼Œä¿¡æ¯çš„èåˆæ€§æ¯”è¾ƒå¥½

>ä½†attentionå¯¹æ•°æ®çš„å‡è®¾åšçš„æ›´å°‘ï¼Œéœ€è¦æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤šçš„æ•°æ®æ‰èƒ½è®­ç»ƒå¤„å’ŒRNNï¼ŒCNNåŒæ ·çš„æ•ˆæœï¼Œå› æ­¤ç›®å‰åŸºäºTransformerçš„æ¨¡å‹é€šå¸¸æ¯”è¾ƒå¤§



### 2.4 ç°æœ‰æœºå™¨ç¿»è¯‘ç ”ç©¶æ–¹æ³•

åœ¨Transformeræ¨¡å‹æå‡ºä¹‹åï¼Œæœºå™¨ç¿»è¯‘é¢†åŸŸçš„ç ”ç©¶æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›å’Œä¼˜åŒ–Transformeræ¨¡å‹åŠå…¶å„ä¸ªç»„ä»¶

åŒ…æ‹¬æ”¹è¿›æ¨¡å‹ç»“æœï¼Œä½¿ç”¨å¤šå±‚å’Œæ·±å±‚çš„æ¨¡å‹ï¼Œè¿›è¡Œæ¨¡å‹çš„å‹ç¼©å’ŒåŠ é€Ÿï¼Œæ”¹è¿›æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¹å–„åºåˆ—å»ºæ¨¡æŠ€æœ¯ç­‰ã€‚



## ä¸‰ã€æ¨¡å‹æ¶æ„

Transformerçš„æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š

![image-20240609190404694](./assets/image-20240609190404694.png)

encoderå°†ç¬¦å·è¡¨ç¤ºçš„è¾“å…¥åºåˆ— $(x_1,...,x_n)$æ˜ å°„æˆä¸€ä¸ªè¿ç»­è¡¨ç¤ºçš„åºåˆ— $z = (z_1,...,z_n)$

ç»™å®šzï¼Œdecoderå°†ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªå­—ç¬¦ï¼Œé€æ¸å°†æ‰€æœ‰çš„è¾“å‡ºåºåˆ— $(y_1,...,y_m)$ç”Ÿæˆï¼Œæ¯æ¬¡åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªå­—ç¬¦æ—¶ï¼Œå°†ä¹‹å‰ç”Ÿæˆçš„ç¬¦å·ä½œä¸ºé™„åŠ è¾“å…¥

>è¿™ç§æ¨¡å‹å«*è‡ªå›å½’çš„ï¼ˆautoregressive modelsï¼‰*
>
>åºåˆ—éœ€è¦é¢„æµ‹ $P(x_t|x_{t-1},...x_1)$å³$x_t$ä¹‹å‰çš„æ‰€æœ‰çš„è¾“å…¥éƒ½å¯¹$P(x_t)$æœ‰å½±å“ï¼Œä½†åœ¨ç›¸å½“é•¿çš„åºåˆ—ä¸­$x_{t-1},..,x_1$å¯èƒ½æ˜¯ä¸å¿…è¦çš„ï¼Œæˆ‘ä»¬åªéœ€è¦æ»¡è¶³æŸä¸ªé•¿åº¦ä¸º$\tau$çš„æ—¶é—´è·¨åº¦å°±å¯ä»¥äº†ï¼Œå³ä½¿ç”¨è§‚æµ‹åºåˆ— $x_{t-1},...,x_{t-\tau}$ï¼Œè¿™å°±æ˜¯è‡ªå›å½’çš„
>
>è¿™é‡Œè®ºæ–‡ä¸­æ‰€è¯´çš„auto-regressiveæ˜¯ä¸€ç§å¹¿ä¹‰çš„ï¼Œå³å½“å‰æ•°æ®çš„äº§ç”Ÿç”¨åˆ°å…ˆå‰çš„æ•°æ®



### 3.1 word embedding åµŒå…¥å±‚

![image-20240609195631457](./assets/image-20240609195631457.png)

#### 3.1.1 åŸç†

word embeddingæ˜¯å°†è¾“å…¥åºåˆ—ä¸­çš„å•è¯è½¬æ¢ä¸ºè¿ç»­å‘é‡è¡¨ç¤ºï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å¤„ç†å’Œç†è§£æ•°æ®ä¸­çš„è¯­ä¹‰ä¿¡æ¯

æœ‰ä¸¤ä¸ªæ­¥éª¤

- è¯å…ƒåŒ–tokenize+å»ºç«‹è¯æ±‡è¡¨vocabulary
- åµŒå…¥å±‚ï¼šå°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå•è¯ç´¢å¼•æ˜ å°„åˆ°æƒé‡çŸ©é˜µä¸­çš„æ¯ä¸€è¡Œï¼Œå¾—åˆ°å¯¹åº”çš„embeddingå‘é‡

>embeddingæ˜¯å°†é«˜ç»´ç©ºé—´çš„ä¸€ä¸ªå‘é‡æŠ•å°„åˆ°ä½ä½ç©ºé—´
>
>- å¦‚æœå°†çŸ©é˜µè¿ç®—çœ‹åšç©ºé—´åæ ‡ç³»çš„å˜åŒ–ï¼Œæƒé‡çŸ©é˜µå°±æ˜¯ä¸¤ä¸ªåæ ‡ç³»é—´æ˜ å°„çš„æ ‡å‡†
>
>å°†tokenç»è¿‡åµŒå…¥çŸ©é˜µæŠ•å°„åˆ°æ½œç©ºé—´ï¼Œè€Œæ½œç©ºé—´æ˜¯ä¸€ä¸ªè¯­ä¹‰ç©ºé—´ï¼Œå¯ä»¥è¡¡é‡ä¸¤ä¸ªä¸åŒçš„tokenä¹‹é—´è¯­ä¹‰çš„å·®åˆ«ï¼Œå› æ­¤å¯ä»¥è®©åç»­çš„æ¨¡å‹å»å¤„ç†
>
>æœºå™¨ç¿»è¯‘ä»»åŠ¡æ˜¯éœ€è¦å¾—åˆ°ä¸¤ç§ä¸åŒè¯­è¨€çš„æ½œç©ºé—´ï¼Œåœ¨ç»Ÿä¸€çš„æ½œç©ºé—´ä¸Šæ‰¾åˆ°è¯­ä¹‰ä¹‹é—´çš„è”ç³»ä»è€Œå®ç°ç¿»è¯‘ï¼Œä¿æŒè¯­ä¹‰ä¸€è‡´
>
>åµŒå…¥ä¹‹åï¼Œæ¯ä¸€ä¸ªå‘é‡çš„ç»´åº¦ä»£è¡¨äº†ä¸€ä¸ªç‹¬ç«‹çš„åŸºç¡€è¯­ä¹‰ï¼Œä»æ•°å­¦çš„è§’åº¦ï¼ŒæŸä¸ªç»´åº¦ä¸Šå€¼è¶Šå¤§å¯èƒ½è¯´æ˜è¿™ä¸ªå‘é‡æ›´æ¥è¿‘æŸç§è¯­ä¹‰

å› æ­¤ï¼Œç»è¿‡word embeddingä¹‹åï¼Œå°±å¯ä»¥å°†åŸå§‹åºåˆ—è¾“å…¥è½¬åŒ–ä¸ºå¯ä»¥è¡¨è¾¾è¯­ä¹‰çš„tensorï¼Œä»è€Œè¿›è¡Œåç»­è®­ç»ƒ

è€Œè¿™ä¸¤ä¸ªåµŒå…¥çŸ©é˜µä¹Ÿæ˜¯éœ€è¦å­¦ä¹ çš„



#### 3.1.2 å®ç°

```python
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
```

å‰å‘ä¼ æ’­æ–¹æ³•ä¸­å°†åµŒå…¥å‘é‡ä»¥ $\sqrt{d_{model}}$è¿›è¡Œç¼©æ”¾ï¼Œä»è€Œå°†åµŒå…¥å‘é‡çš„æ•°å€¼èŒƒå›´è°ƒæ•´åˆ°åˆé€‚æ¨¡å‹è®­ç»ƒçš„åˆå§‹é˜¶æ®µ

åœ¨`EncoderDecoder`æ¨¡å—ä¸­å¯¹è¾“å…¥çš„åºåˆ—æ•°æ®ç›´æ¥è¿›è¡Œembeddingï¼Œä»è€Œå¯ä»¥è¿›å…¥åç»­æ¨¡å‹è®¡ç®—



### 3.2 encoder-decoder ç»“æ„

#### 3.2.1 ä¿ç•™åŸå› 

åœ¨äººç±»è¿›è¡Œè¯­è¨€ç¿»è¯‘ä¸­ï¼Œå¸¸å°†ä¸€ä¸¤ç§è¯­è¨€å¯¹åº”äºåŒä¸€ä¸ªå®ä½“

>æ¯”å¦‚ä¸­æ–‡çš„è‹¹æœï¼Œå’Œè‹±æ–‡çš„appleï¼Œä¼šå’Œè¿™ä¸ªå…·ä½“çš„æ°´æœå¯¹åº”ï¼Œä»è€Œè”ç³»è¯­ä¹‰

è€Œæœºå™¨ç¿»è¯‘ä¸­ï¼Œæˆ‘ä»¬æ— æ³•æ‰¾åˆ°è¿™æ ·ä¸€ç§å®ä½“ä¾›æœºå™¨å»å­¦ä¹ ï¼Œåªèƒ½ç”¨çº¯æ–‡æœ¬åšå¯¹åº”ï¼Œéœ€è¦é€šè¿‡å¤§é‡æ–‡æœ¬çš„ä¸Šä¸‹æ–‡å»ç¡®å®šè¯è¯­è¯ä¹‹é—´çš„å…³ç³»

encoderå’Œdecoderä¸­çš„codeå³æ˜¯ è¯­ä¹‰å…³ç³»ï¼Œéœ€è¦æ•°å­—åŒ–ä¹‹åï¼Œé€šè¿‡ä¸€ç§ç­–ç•¥æå‰ä¸åŒtokenä¹‹é—´çš„å¯¹åº”å…³ç³»

>æ¯”å¦‚one-hotç‹¬çƒ­ç¼–ç å’Œtokenizer

Transformerä¾ç„¶ä¿æŒ encoder-decoderç»“æ„

>å¼•å…¥encoder-decoderçš„ç»“æ„æ˜¯ä¸ºäº†è§£å†³æœºå™¨ç¿»è¯‘çš„seq2seqé—®é¢˜ä¸­ï¼Œè¾“å…¥è¯­å¥çš„é•¿çŸ­å’Œè¾“å‡ºè¯­å¥çš„é•¿çŸ­ä¸åŒ
>
>ä¸ºäº†å¤„ç†è¿™ç§ç±»å‹çš„è¾“å…¥å’Œè¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡encoderæ¥å—ä¸€ä¸ªé•¿åº¦å¯å˜çš„åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå…·æœ‰å›ºå®šå½¢çŠ¶çš„ç¼–ç çŠ¶æ€
>
>é€šè¿‡decoderï¼Œå°†å›ºå®šæƒ³åˆç§Ÿé‚£ä¸ªçš„ç¼–ç çŠ¶æ€æ˜ å°„åˆ°é•¿åº¦å¯å˜çš„åºåˆ—
>
>ä»è€Œå®ç°seq2seqä¸­è¾“å…¥è¾“å‡ºé•¿åº¦å¯å˜



#### 3.2.2 ç»“æ„åŸç†

![image-20240609174727768](./assets/image-20240609174727768.png)

##### Encoder

Encoderæ˜¯ç”±6ä¸ªå®Œå…¨ç›¸åŒçš„layerå †å è€Œæˆï¼Œæ¯å±‚æœ‰ä¸¤ä¸ªå­å±‚

- multi-head self-attentionï¼ˆå¤šå¤´è‡ªæ³¨æ„åŠ›ï¼‰
- å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆå®é™…ä¸Šæ˜¯ä¸€ä¸ªmlpï¼‰

ä¸¤ä¸ªå­å±‚çš„æ¯ä¸€å±‚åé‡‡ç”¨æ®‹å·®è¿æ¥ï¼ˆç±»ä¼¼Resnetï¼‰ï¼Œç„¶åè¿›è¡Œlayer normalization

>layer normalization ä¸ batch normalizationçš„åŒºåˆ«
>
>layer normalizationæ˜¯åœ¨å±‚ä¸Šåšå½’ä¸€åŒ–ï¼Œè€Œbatch normalizationæ˜¯åœ¨æ‰¹é‡ä¸Šåšå½’ä¸€åŒ–
>
>BNï¼š
>
>å¯¹äºmini-batchä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œè®¡ç®—å‡å€¼å’Œæ–¹å·®å¹¶æ˜ å°„åˆ° N~(0, 1)
>
>LNï¼š
>
>å¯¹äºæ¯ä¸€ä¸ªlayerä¸Šçš„è¾“å…¥ï¼Œè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
>
>å¯¹äºä¸€ä¸ªä¸‰ç»´çš„tensor
>
>![image-20240609191523012](./assets/image-20240609191523012.png)
>
>åœ¨æŒç»­çš„åºåˆ—ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬çš„é•¿åº¦ï¼ˆsequenceé•¿åº¦ä¸åŒï¼‰ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå¦‚æœæ¯æ¬¡ä»¥ä¸€ä¸ªbatchåšåˆ‡é¢ï¼Œé‚£ä¹ˆæ¯ä¸ªfeatureä¹‹é—´çš„ç©ºé—²çš„ä½ç½®éœ€è¦ç”¨ç©ºå¡«å……ï¼Œåšå½’ä¸€åŒ–æ—¶ä¼šæœ‰å¾ˆå¤šå¡«å……çš„æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œåœ¨æ ·æœ¬é•¿åº¦å˜åŒ–æ¯”è¾ƒå¤§çš„æ—¶å€™ï¼Œè®¡ç®—çš„æ˜¯å…¨å±€çš„å‡å€¼å’Œæ–¹å·®ï¼Œå› æ­¤ç®—å‡ºæ¥çš„æ–¹å·®å’Œå‡å€¼å·®åˆ«æ¯”è¾ƒå¤§ã€‚
>
>è€Œlayer normå¯¹äºæ¯ä¸ªæ ·æœ¬æ±‚å‡å€¼å’Œæ–¹å·®ï¼Œä¸å…¶ä»–çš„æ ·æœ¬æ— å…³ï¼Œæ‰€ä»¥sequenceé•¿åº¦å˜åŒ–å¯¹å‡å€¼å’Œæ–¹å·®æ²¡æœ‰å¤ªå¤šå½±å“
>
>å› æ­¤å¯¹äºåºåˆ—è¾“å…¥ï¼Œç”¨layer normåšå½’ä¸€åŒ–ä¼šå¥½ä¸€ç‚¹

##### Decoder

ä¹Ÿç”±N = 6ä¸ªå®Œå…¨ç›¸åŒçš„layerå †å è€Œæˆï¼Œé™¤äº†æ¯ä¸ªç¼–ç å™¨å±‚ä¸­çš„ä¸¤ä¸ªå­å±‚ä¹‹å¤–ï¼Œè§£ç å™¨è¿˜æ’å…¥äº†ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œå¯¹ç¼–ç å™¨çš„è¾“å‡ºæ‰§è¡Œ multi-head attention

æ¯ä¸ªå­å±‚åé¢ä¹Ÿä½¿ç”¨æ®‹å·®è¿æ¥å’Œ layer normalization



#### 3.2.3 ç»“æ„å¤ç°

encoderå’Œdecoderæ¥æ”¶çš„æ˜¯inputå’Œouputç»è¿‡embeddingçš„æ•°æ®ã€‚

##### Encoder

```python
class Encoder(nn.Module):
    "Core encoder is a stack of N layers"

    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        "Pass the input (and mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

- å°†layeræ¨¡å—clone Nè¯å¾—åˆ°ä¸€ä¸ªåŒ…å«Nä¸ªç›¸åŒå±‚çš„åˆ—è¡¨
- é‡‡ç”¨ layer normåšå±‚å½’ä¸€åŒ–
- å‰å‘ä¼ æ’­ç”¨ä¸€ä¸ªmaskå±è”½æ— æ•ˆçš„å¡«å……éƒ¨åˆ†ï¼Œæˆ–è€…æœªæ¥ä¿¡æ¯ï¼Œç„¶åé€å±‚ä¼ é€’

##### Decoder

```python
class Decoder(nn.Module):
    "Generic N layer decoder with masking."

    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
```

- `memory`æ˜¯ä»ç¼–ç å™¨å¾—åˆ°çš„è¾“å‡º
- `src_mask`æ˜¯æºè¯­è¨€åºåˆ—çš„æ©ç 
- `tgt_mask`æ˜¯ç›®æ ‡è¯­è¨€åºåˆ—çš„æ©ç 



### 3.3 Attention æ³¨æ„åŠ›æœºåˆ¶

å¯¹transformeræ¨¡å‹è€Œè¨€ï¼Œattentionæœºåˆ¶æ˜¯è¿™ä¸ªæ¨¡å‹æœ€é‡è¦çš„æœºåˆ¶ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸¤ç§attention

- scaled Dot-Product Attention
- Multi-Head Attention

![image-20240609195452992](./assets/image-20240609195452992.png)

å¹¶åœ¨3å¤„ä½¿ç”¨åˆ°äº†attentionæœºåˆ¶



#### 3.3.1 åŸç†

æ³¨æ„åŠ›æ˜¯ä¸€ä¸ªqueryå’Œä¸€äº›key-valueså¯¹çš„æ˜ å°„ï¼Œè¾“å‡ºå°±æ˜¯queryåœ¨ä¸åŒkeyä¸Šçš„æƒå€¼å’Œ

>æ¯”å¦‚queryåœ¨5ä¸ªkeyä¸Šï¼Œä¸$k_2,k_3$æ¯”è¾ƒç›¸ä¼¼ï¼Œå¯¹åº” $v_2,v_3$ä¼šå¤§ä¸€äº›ï¼Œå…¶ä»–çš„åå°

æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®¡ç®—ä¸€ç»„æ³¨æ„åŠ›æƒé‡ï¼ˆattention weightsï¼‰ï¼Œè¿™äº›æƒé‡è¡¨ç¤ºè¾“å…¥åºåˆ—çš„å„ä¸ªéƒ¨åˆ†å¯¹å½“å‰è¾“å‡ºçš„é‡è¦ç¨‹åº¦ã€‚ç„¶åï¼Œé€šè¿‡åŠ æƒæ±‚å’Œçš„æ–¹å¼å°†è¿™äº›é‡è¦éƒ¨åˆ†çš„è¡¨ç¤ºèšåˆèµ·æ¥ï¼Œç”Ÿæˆå½“å‰æ—¶åˆ»çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontext vectorï¼‰ã€‚

![image-20240609201757329](./assets/image-20240609201757329.png)

å¯¹äºæ¯ä¸€ä¸ªæŸ¥è¯¢ $q \in R^q$ï¼Œmä¸ªé”®å€¼å¯¹ $(k_1,v_1),...,(k_m,v_m)$

æ³¨æ„åŠ›æ±‡èšå‡½æ•°fè¢«è¡¨ç¤ºæˆå€¼çš„åŠ æƒå’Œ

$f(q,(k_1,v_1),...,(k_m,v_m)) = \sum_{i=1}^{m}\alpha(q,k_i)v_i \in R^v$

è€ŒæŸ¥è¯¢qå’Œé”®$k_i$çš„æ³¨æ„åŠ›æƒé‡æ˜¯é€šè¿‡æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°aå°†ä¸¤ä¸ªå‘é‡æ˜ å°„æˆæ ‡é‡ï¼Œå†è®²è¿‡softmaxè¿ç®—å¾—åˆ°
$$
\alpha(q,k_i) = softmax(a(q,k_i)) = \frac{\exp(a(q,k_i))}{\sum_{j=1}^m \exp(a(q,k_j))} \in  R
$$



#### 3.3.2  scaled Dot-Product Attention

ä½¿ç”¨ç‚¹ç§¯å¯ä»¥å¾—åˆ°è®¡ç®—æ•ˆç‡æ›´é«˜çš„è¯„åˆ†å‡½æ•°ï¼Œç‚¹ç§¯æ“ä½œè¦æ±‚æŸ¥è¯¢å’Œé”®å…·æœ‰ç›¸åŒçš„é•¿åº¦ğ‘‘

![image-20240609200009393](./assets/image-20240609200009393.png)

æ³¨æ„åŠ›æœºåˆ¶è¦è§£å†³çš„æ˜¯è®¸å¤šè¯å‡ºç°åœ¨ä¸€èµ·ä¹‹åæ•´ä½“è¡¨ç°çš„è¯­ä¹‰ï¼Œå› æ­¤éœ€è¦å°†å¤šä¸ªè¯åŒæ—¶è¾“å…¥åˆ°æ¨¡å‹ï¼Œå³è¾“å…¥çŸ©é˜µDæ˜¯ä¸€ä¸ªå¤šè¡Œçš„çŸ©é˜µ

å°†queryå†™æˆä¸€ä¸ªçŸ©é˜µ $Q$ï¼Œkeyå’Œvalueåˆ†åˆ«å†™æˆ $K$å’Œ $V$

è¾“å…¥çŸ©é˜µéœ€è¦å’Œä¸‰ä¸ªçŸ©é˜µ $W_q, W_k,W_v$çŸ©é˜µæƒ³è±¡æˆï¼Œå¾—åˆ°ä¸‰ä¸ªçŸ©é˜µ $Q,K,V$ç”¨äºåç»­softmaxè®¡ç®—
$$
Attention(Q,K,V) = softmax(\frac{QK^t}{\sqrt{d_k}})V
$$
$Q \cdot K^T$ï¼Œç›¸å½“äºè®¡ç®—è¾“å…¥çš„è¯å‘é‡ä¸­ä¸¤ä¸¤ä¹‹é—´çš„ç›¸å…³å…³ç³»ï¼Œç»è¿‡softmaxå’Œç¼©æ”¾åå˜æˆ(0, 1)ä¹‹é—´çš„å€¼ï¼Œç„¶åç”¨è¿™ä¸ªå¾—åˆ°çš„çŸ©é˜µå»ä¿®æ­£è¾“å…¥çš„è¯å‘é‡çŸ©é˜µVçš„æ¯ä¸€ä¸ªç»´åº¦ï¼Œå½“ $softmax(\frac{Q \cdot K^T}{\sqrt{D_{out}}})$è®¡ç®—å‡ºçš„çŸ©é˜µæŸä¸€ä¸ªå€¼ $A^`_{i,j}$æ¯”è¾ƒå¤§ï¼Œè¯´æ˜ç¬¬iä¸ªè¯å‘é‡å’Œç¬¬jä¸ªè¯å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦æ¯”è¾ƒå¤§ï¼Œå°±èƒ½ç”¨è¿™ä¸ªå€¼æ¥ä¿®æ­£åŸå…ˆè¾“å…¥è¯å‘é‡çŸ©é˜µVï¼ˆç»è¿‡ä¸€æ¬¡å˜æ¢ï¼‰çš„æ¯ä¸€ç»´ï¼Œä»è€Œè¡¨ç°å‡ºè¯­ä¹‰çš„ç›¸ä¼¼æ€§

ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶æ¯”åŠ æ€§æ³¨æ„åŠ›æœºåˆ¶çš„ç©ºé—´æ•ˆç‡æ›´é«˜ï¼ŒåŒæ—¶å¿«å¾—å¤šï¼Œå› ä¸ºå¯ä»¥ç”¨é«˜åº¦ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•ä»£ç æ¥å®ç°

è€Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶é™¤ä»¥ $\sqrt{d_k}$æ˜¯ä¸ºäº†è®©softmaxçš„ç»“æœä¸è¦å¤ªå‘ä¸¤ç«¯é æ‹¢

å¯¹äº$d_k$æ¯”è¾ƒå¤§æ—¶ï¼Œsoftmaxçš„ç»“æœä¼šè®©ç‚¹ç§¯çš„å€¼æ¯”è¾ƒå¤§æˆ–è€…æ¯”è¾ƒå°ï¼Œç»è¿‡softmaxä¹‹åæ¥è¿‘1æˆ–è€…æ¥è¿‘0ï¼Œè¿™æ ·ä¼šå¯¼è‡´æ¢¯åº¦æ¯”è¾ƒå°ï¼Œè¿è¡Œå˜æ…¢ï¼Œå› æ­¤é€‰æ‹©ç¼©æ”¾è€Œä¸æ˜¯ç›´æ¥ç”¨ç‚¹å‡»æ³¨æ„åŠ›æœºåˆ¶

è€Œmaskæ˜¯ä¸ºäº†é¿å…åœ¨å½“å‰æ—¶åˆ»çœ‹åˆ°ä»¥åæ—¶åˆ»çš„è¾“å…¥ï¼Œå› æ­¤åœ¨scaleä¹‹ååšäº†ä¸€æ¬¡mask

>å¯¹äº$q_t$ï¼Œåº”è¯¥åªçœ‹åˆ° $k_1,...,k_{t-1}$ï¼Œä¸è¦ç”¨åˆ°åé¢çš„è¾“å…¥ï¼Œä½†æ˜¯è¾“å…¥æ˜¯å°†æ‰€æœ‰çš„téƒ½è¾“å…¥ï¼Œæ‰€ä»¥éœ€è¦ç”¨maskå°†åé¢çš„æƒé‡å˜æˆ0ï¼Œåªä¼šè®©tä¹‹é—´çš„å€¼æœ‰æ•ˆæœ



#### 3.3.3 Multi-Head Attention

![image-20240609203811700](./assets/image-20240609203811700.png)

å½“ç»™å®šç›¸åŒçš„æŸ¥è¯¢ã€é”®å’Œå€¼çš„é›†åˆæ—¶ï¼Œ æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å¯ä»¥åŸºäºç›¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ åˆ°ä¸åŒçš„è¡Œä¸ºï¼Œ ç„¶åå°†ä¸åŒçš„è¡Œä¸ºä½œä¸ºçŸ¥è¯†ç»„åˆèµ·æ¥ï¼Œ æ•è·åºåˆ—å†…å„ç§èŒƒå›´çš„ä¾èµ–å…³ç³»

dot-producté‡Œé¢å¯å­¦çš„å‚æ•°æ¯”è¾ƒå°‘ï¼Œè€Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ç”¨åˆ°æŠ•å½±ï¼Œå¯ä»¥å»å­¦ä¹ ä¸åŒæ¨¡å¼å»åŒ¹é…çš„ç›¸ä¼¼å‡½æ•°

æœ€åå°†å¤šä¸ªè¾“å‡ºè¿æ¥èµ·æ¥é€šè¿‡ä¸€ä¸ªmlp

è®ºæ–‡ä¸­å®é™…ç”¨åˆ°çš„h = 8ï¼Œæ¯æ¬¡è®²ä¸€ä¸ª512ç»´çš„è¾“å…¥ï¼ˆembeddingä¹‹åï¼‰æŠ•å½±åˆ°ä¸€ä¸ª64ä½çš„è¾“å‡ºï¼Œç»è¿‡æ³¨æ„åŠ›æœºåˆ¶åï¼Œå†è¿æ¥é€šè¿‡mlp

è®¡ç®—å…¬å¼
$$
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O\\
where~head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)
$$
ç›¸å…³ä»£ç å®ç°å¦‚ä¸‹

```python
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        "Take in model size and number of heads."
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = [
            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for lin, x in zip(self.linears, (query, key, value))
        ]

        # 2) Apply attention on all the projected vectors in batch.
        x, self.attn = attention(
            query, key, value, mask=mask, dropout=self.dropout
        )

        # 3) "Concat" using a view and apply a final linear.
        x = (
            x.transpose(1, 2)
            .contiguous()
            .view(nbatches, -1, self.h * self.d_k)
        )
        del query
        del key
        del value
        return self.linears[-1](x)
```



#### 3.3.4 è®ºæ–‡ä¸­çš„åº”ç”¨

**æ³¨æ„åŠ›å±‚**ï¼ˆä¸‰ç§ï¼‰

* **encoderä¸­çš„multiple-Head Attention**

  è¾“å…¥è¡¨ç¤ºçš„æ˜¯ keyï¼Œvalueï¼Œqueryï¼ŒåŒæ—¶keyï¼Œvalueå’Œqueryæ˜¯åŒä¸€ä¸ªè¾“å…¥ï¼Œå³è‡ªæ³¨æ„åŠ›æœºåˆ¶

  è¾“å‡ºæ˜¯ä¸€ä¸ªæƒé‡å’Œï¼Œä»£è¡¨æ¯ä¸ªå‘é‡å’Œåˆ«çš„å‘é‡çš„ç›¸ä¼¼åº¦

  ç”±äºè¿™é‡Œé‡‡ç”¨çš„æ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå› æ­¤ä¼šå­¦ä¹ å‡ºhä¸ªä¸åŒçš„æŠ•å½±

* **decoderä¸­çš„masked multi-head attention**

  ä¹Ÿæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è§£ç å™¨ä¸­å­˜åœ¨ä¸€ä¸ªmaskï¼Œä½¿å¾—å½“å‰æ—¶åˆ» $q_t$åªä¼šçœ‹åˆ°å‰t-1çš„ä¿¡æ¯ï¼Œå¿½ç•¥åé¢çš„ä¿¡æ¯

* **decoderä¸­çš„Multi-head attention**

**éè‡ªæ³¨æ„åŠ›å±‚**

keyå’Œvalueæ¥è‡ªäºencoderçš„è¾“å‡ºï¼Œè€Œqueryæ¥è‡ªdecoderä¸Šä¸€ä¸ªmasked attentionçš„è¾“å…¥

å¯¹è§£ç å™¨çš„æ¯ä¸€ä¸ªè¾“å‡ºqueryï¼Œéœ€è¦è®¡ç®—å’Œç¼–ç å™¨çš„è¾“å‡ºçš„ç›¸ä¼¼åº¦

è¿™é‡Œå°±ä½“ç°å‡ºè¾“å‡ºçš„ç¿»è¯‘æ–‡æœ¬å’Œè¾“å…¥çš„æ–‡æœ¬çš„ç›¸ä¼¼åº¦

>æ¯”å¦‚è‹±æ–‡hello worldç¿»è¯‘ä¸­æ–‡ ä½ å¥½ ä¸–ç•Œï¼Œå¯¹äºè§£ç å™¨çš„ä¸­çš„ ä½  å¥½è¿™ä¸¤ä¸ªqueryä¼šè·Ÿç¼–ç å™¨ä¸­çš„ hello çš„æƒé‡æ¯”è¾ƒå¤§



### 3.4 Position-wise Feed-Forward Networks

å®é™…ä¸Šæ˜¯ä¸€ä¸ªä¸¤å±‚çº¿æ€§å±‚çš„mlpï¼ˆå…¨è¿æ¥å±‚ï¼‰ï¼Œè€ƒè™‘æ³¨æ„åŠ›æœºåˆ¶å¯èƒ½å¯¹å¤æ‚è¿‡ç¨‹çš„æ‹Ÿåˆç¨‹åº¦ä¸å¤Ÿ, é€šè¿‡å¢åŠ ä¸¤å±‚ç½‘ç»œæ¥å¢å¼ºæ¨¡å‹çš„èƒ½åŠ›ï¼Œå®ç°ä¸€ç§è¯­ä¹‰æ±‡èš

æŠŠåŒä¸€ä¸ªmlpå¯¹æ¯ä¸€ä¸ªè¾“å…¥åºåˆ—ä¸­çš„tokenä½œç”¨ä¸€æ¬¡
$$
FFN(x) = max(0,xW_1 +b_1)W_2 +b_2
$$

- å…ˆå¯¹è¾“å…¥è¿›è¡Œä¸€ä¸ªçº¿æ€§å˜æ¢

  $x' = W_1x + b_1$

- éçº¿æ€§æ¿€æ´»å‡½æ•°ReLU

  $x'' = ReLU(x')$

- ç¬¬äºŒæ¬¡çº¿æ€§å˜æ¢

  $y = W_2x'' + b_2$

FFNæ˜¯æŒ‰æ¯ä¸ªä½ç½®ç‹¬ç«‹è¿›è¡Œçš„ï¼Œåœ¨å¤„ç†è¾“å…¥åºåˆ—æ—¶ï¼Œæ¯ä¸ªä½ç½®çš„è¾“å…¥å‘é‡éƒ½ç»è¿‡ç›¸åŒçš„å‰é¦ˆç½‘ç»œè¿›è¡Œå˜æ¢ã€‚å¯¹è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤º

ç›¸å…³ä»£ç å®ç°å¦‚ä¸‹

```python
class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."

    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(self.w_1(x).relu()))
```

ç»è¿‡attentionä¹‹åï¼Œåºåˆ—ä¿¡æ¯å·²ç»è¢«æ±‡èšå®Œæˆäº†ï¼Œå› æ­¤mlpåªéœ€è¦å¯¹æ¯ä¸ªå•ç‹¬çš„ç‚¹è¿ç®—å°±å¯ä»¥äº†

è€ŒRNNï¼Œéœ€è¦ä¼ é€’åºåˆ—ä¿¡æ¯ï¼Œåœ¨æ¯ä¸€ä¸ªmlpä¸ä»…æ¥å—è¾“å…¥çš„åºåˆ—ï¼Œè¿˜éœ€è¦ä¸Šä¸€ä¸ªæ—¶åˆ»çš„mlpçš„è¾“å‡ºï¼Œä¸€èµ·ä½œç”¨äºä¸€ä¸ªmlpå±‚ï¼Œä»è€Œä¼ é€’åºåˆ—ä¹‹é—´çš„æ—¶åºä¿¡æ¯ï¼ˆæŠŠä¸Šä¸€ä¸ªæ—¶åˆ»çš„è¾“å‡ºä¼ å…¥ä¸‹ä¸€ä¸ªæ—¶åˆ»åšè¾“å…¥ï¼‰ã€‚



### 3.5 Positional Encoding

attentionæœºåˆ¶å¹¶æ²¡æœ‰æ—¶åºä¿¡æ¯ï¼Œè¾“å‡ºæ˜¯valueçš„åŠ æƒå’Œï¼Œä¸åºåˆ—ä¿¡æ¯æ— å…³ã€‚å°†ä¸€ä¸ªåºåˆ—ä»»æ„æ‰“ä¹±ï¼Œattentionæ— æ³•å¯Ÿè§‰è¿™ç§å˜åŒ–ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›ç½‘ç»œå¯ä»¥å¯Ÿè§‰åˆ°è¿™ç§å˜åŒ–

å› æ­¤positional encodingåœ¨è¾“å…¥é‡ŒåŠ å…¥äº†æ—¶åºçš„ä¿¡æ¯ï¼Œæ¯ä¸ªè¯åœ¨ä¸åŒçš„ä½ç½®ä¼šæœ‰çš„ä¸åŒçš„å€¼

æˆ‘ä»¬é‡‡ç”¨fourierå˜æ¢ä¸­çš„sinï¼Œcoså‡½æ•°ï¼Œå¯¹äºä¸€ä¸ª512ç»´åº¦çš„è¾“å…¥å‘é‡ï¼ŒåŠ å…¥ä¸ä½ç½®æœ‰å…³çš„ä¿¡æ¯ï¼Œä»è€Œå°†è¿™ç§æ—¶åºçš„ä¿¡æ¯å‘åç»­ä¼ é€’åˆ°æ¨¡å‹ä¸­å»

å…·ä½“çš„å…¬å¼å¦‚ä¸‹
$$
PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})\\
PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})
$$

- posæ˜¯ä¸åŒtokençš„ä½ç½®
- iæ˜¯ç»´åº¦

è¿™æ ·æ¨¡å‹å°±å¯ä»¥é€šè¿‡ç›¸å¯¹ä½ç½®æ¥å­¦ä¹ åˆ°æ—¶åºä¿¡æ¯

ç›¸å…³ä»£ç å®ç°å¦‚ä¸‹

```python
class PositionalEncoding(nn.Module):
    "Implement the PE function."

    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)
        return self.dropout(x)
```



## å››ã€ è®­ç»ƒç­–ç•¥

### 4.1 è®­ç»ƒç­–ç•¥æ¦‚è¿°

åœ¨Transformeræ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨å¦‚ä½•æœ‰æ•ˆåœ°ä¼˜åŒ–æ¨¡å‹å‚æ•°ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è®­ç»ƒç­–ç•¥åŒ…æ‹¬ï¼šåˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ã€å®šä¹‰æŸå¤±å‡½æ•°ï¼Œä»¥åŠå…·ä½“çš„è®­ç»ƒè¿‡ç¨‹å’Œç»†èŠ‚ã€‚



### 4.2 ç¡¬ä»¶å’Œæ—¶é—´è¡¨

* è®ºæ–‡

è®ºæ–‡åœ¨ä¸€å°æœ‰8ä¸ªNVIDIA P100 GPUçš„æœºå™¨ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨æ–‡ä¸­æ‰€è¿°çš„è¶…å‚æ•°ï¼Œæ¯ä¸ªè®­ç»ƒæ­¥éª¤å¤§çº¦éœ€è¦0.4ç§’ã€‚è®ºæ–‡ä¸­æ€»å…±è®­ç»ƒäº†100,000æ­¥æˆ–12å°æ—¶çš„åŸºç¡€æ¨¡å‹ã€‚å¯¹äºå¤§æ¨¡å‹ï¼Œæ­¥éª¤æ—¶é—´ä¸º1.0ç§’ã€‚å¤§æ¨¡å‹è¢«è®­ç»ƒäº†30ä¸‡æ­¥ï¼ˆ3.5å¤©ï¼‰ã€‚

* å¤ç°

æˆ‘ä»¬å¤ç°è®ºæ–‡ä½¿ç”¨çš„ç¡¬ä»¶è®¾æ–½ä¸ºï¼š

```
========å®ä¾‹é…ç½®========
æ ¸æ•°ï¼š16
å†…å­˜ï¼š30 GB
ç£ç›˜ï¼š23% 6.9G/30G
æ˜¾å¡ï¼šNVIDIA GeForce RTX 3090, 1
```



### 4.3 è®­ç»ƒæ•°æ®å’Œé¢„å¤„ç†

#### 4.3.1 **æ•°æ®é›†ä»‹ç»**

è®ºæ–‡ä½¿ç”¨äº†WMT 2014è‹±è¯­-å¾·è¯­ï¼ˆEnglish-Germanï¼‰è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚è¯¥æ•°æ®é›†æ˜¯æœºå™¨ç¿»è¯‘é¢†åŸŸçš„æ ‡å‡†æ•°æ®é›†ï¼Œå¹¿æ³›ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œæ€§èƒ½æ¯”è¾ƒã€‚ä»æ­¤æ•°æ®é›†ä¸Šæå–äº†*è®­ç»ƒæ•°æ®é›†*ã€*éªŒè¯æ•°æ®é›†*ã€*æµ‹è¯•æ•°æ®é›†*ã€‚å…¶ä¸­è®­ç»ƒæ•°æ®é›†ä¸­åŒ…å«æ•°ç™¾ä¸‡å¯¹è‹±å¾·å¥å­å¯¹ï¼Œè¿™äº›å¥å­å¯¹ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œä»¥ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä»è‹±è¯­åˆ°å¾·è¯­çš„ç¿»è¯‘ã€‚

æœ¬æ¬¡å¤ç°è®ºæ–‡ä½¿ç”¨çš„æ˜¯ Multi30k æ•°æ®é›†ï¼Œå®ƒæ˜¯ä¸€ä¸ªå°å‹çš„è‹±è¯­-å¾·è¯­å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«äº†æ¥è‡ª Flickr30k å›¾åƒæè¿°æ•°æ®é›†çš„è‹±è¯­å¥å­å’Œå®ƒä»¬çš„å¾·è¯­ç¿»è¯‘ã€‚Multi30k æ•°æ®é›†åŒ…å«çº¦3ä¸‡ä¸ªè®­ç»ƒå¥å­ã€1,014ä¸ªéªŒè¯å¥å­å’Œ1,000ä¸ªæµ‹è¯•å¥å­ã€‚ç”±äºå…¶ç›¸å¯¹è¾ƒå°çš„è§„æ¨¡ï¼ŒMulti30kæ•°æ®é›†è¢«å¹¿æ³›ç”¨äºè¯„ä¼°å°å‹ç¿»è¯‘æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿæ˜¯è¿›è¡Œå¿«é€Ÿå®éªŒå’ŒåŸå‹å¼€å‘çš„ç†æƒ³é€‰æ‹©ã€‚



#### 4.3.2 **æ•°æ®é¢„å¤„ç†**

æ•°æ®åœ¨è¾“å…¥åˆ°æ¨¡å‹ä¹‹å‰éœ€è¦ç»è¿‡é¢„å¤„ç†ï¼ŒåŒ…æ‹¬åˆ†è¯ã€æ ‡æ³¨ç­‰ã€‚

* **åŠ è½½åˆ†è¯å™¨**

> åŠ è½½è‹±è¯­å’Œå¾·è¯­çš„ *spaCy* åˆ†è¯å™¨æ¨¡å‹ï¼Œå¦‚æœæ¨¡å‹æœªä¸‹è½½ï¼Œåˆ™é€šè¿‡å‘½ä»¤è¡Œä¸‹è½½æ¨¡å‹ã€‚``spacy_de`` å’Œ``spacy_en``æ˜¯ *spaCy* è¯­è¨€å¤„ç†æµæ°´çº¿å¯¹è±¡ï¼ŒåŒ…å«å¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†çš„å„ç§åŠŸèƒ½ï¼Œå¦‚åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚

```python
def load_tokenizers():
    # åŠ è½½ spaCy åˆ†è¯å™¨æ¨¡å‹ï¼šå¾·è¯­å’Œè‹±è¯­
    try:
        spacy_de = spacy.load("de_core_news_sm")
    except IOError:
        os.system("python -m spacy download de_core_news_sm")
        spacy_de = spacy.load("de_core_news_sm")

    try:
        spacy_en = spacy.load("en_core_web_sm")
    except IOError:
        os.system("python -m spacy download en_core_web_sm")
        spacy_en = spacy.load("en_core_web_sm")

    return spacy_de, spacy_en
```

* **åˆ†è¯**

> ``tokenize(text, tokenizer)`` å‡½æ•°å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼›å¹¶ç”Ÿæˆåˆ†è¯åçš„ä»¤ç‰Œåºåˆ—ï¼Œç”¨äºæ„å»ºè¯æ±‡è¡¨ã€‚
>
> `yield_tokens(data_iter, tokenizer, index)` å‡½æ•°æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå®ƒæ¥å—æ•°æ®è¿­ä»£å™¨å’Œåˆ†è¯å™¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®ç´¢å¼•å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†å’Œåˆ†è¯ã€‚

```python
def tokenize(text, tokenizer):      
    # åˆ†è¯ï¼Œtextæ˜¯æ–‡æœ¬å­—ç¬¦ä¸²
    # tokenizeræ˜¯ç”¨äºåˆ†è¯çš„spaCyè¯­è¨€å¤„ç†å¯¹è±¡ï¼ˆåˆ†è¯å™¨ï¼‰
    return [tok.text for tok in tokenizer.tokenizer(text)]

def yield_tokens(data_iter, tokenizer, index):  
    # ç”Ÿæˆå™¨ï¼Œdata_iteræ˜¯æ•°æ®è¿­ä»£å™¨ï¼Œtokenizeræ˜¯åˆ†è¯å™¨ï¼Œindexæ˜¯ç´¢å¼•
    for from_to_tuple in data_iter:
        yield tokenizer(from_to_tuple[index])
```

* **æ„å»ºè¯æ±‡è¡¨**

  å®šä¹‰``build_vocabulary(spacy_de, spacy_en)`` å‡½æ•°ï¼š

  ```python
  def build_vocabulary(spacy_de, spacy_en)
  ```

  * å®šä¹‰åˆ†è¯æ–¹æ³•

    ```python
    def tokenize_de(text):
        return tokenize(text, spacy_de)
        def tokenize_en(text):
            return tokenize(text, spacy_en)
    ```

  * ä½¿ç”¨`torchtext`åº“ä¸­çš„`Multi30k`æ¨¡å—åŠ è½½æ•°æ®é›†

    ```python
        print("Building German Vocabulary ...")
        train, val, test = datasets.Multi30k(language_pair=("de", "en"))    # åŠ è½½æ•°æ®é›†
    ```

  * ä½¿ç”¨`torchtext`åº“çš„`build_vocab_from_iterator`å‡½æ•°æ„å»ºè¯æ±‡è¡¨ï¼Œå¹¶æ·»åŠ ç‰¹æ®Šæ ‡è®°`<s>`, `</s>`, `<blank>`, `<unk>`

    ```python
        vocab_src = build_vocab_from_iterator(
            yield_tokens(train + val + test, tokenize_de, index=0),
            min_freq=2,
            specials=["<s>", "</s>", "<blank>", "<unk>"],
        )   # æ„å»ºå¾·è¯­è¯æ±‡è¡¨
    ```

    > ä»¥å¾·è¯­è¯æ±‡è¡¨ä¸ºä¾‹ï¼Œè‹±è¯­è¯æ±‡è¡¨åŒç†ã€‚

  * è®¾ç½®é»˜è®¤ç´¢å¼•åºåˆ—

    ```python
        # è®¾ç½®é»˜è®¤ç´¢å¼•
        vocab_src.set_default_index(vocab_src["<unk>"])     
        vocab_tgt.set_default_index(vocab_tgt["<unk>"])
    
        return vocab_src, vocab_tgt
    ```



> ä¸ºäº†èŠ‚çœæ—¶é—´å¹¶é¿å…é‡å¤æ„å»ºè¯æ±‡è¡¨ï¼Œ``load_vocab(spacy_de, spacy_en)`` å‡½æ•°å°†è¯æ±‡è¡¨ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œå¹¶åœ¨éœ€è¦æ—¶åŠ è½½ã€‚

```python
# åŠ è½½è¯æ±‡è¡¨
def load_vocab(spacy_de, spacy_en):
    if not exists("vocab.pt"):
        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)
        torch.save((vocab_src, vocab_tgt), "vocab.pt")
    else:
        vocab_src, vocab_tgt = torch.load("vocab.pt")
    print("Finished.\nVocabulary sizes:")
    print(len(vocab_src))
    print(len(vocab_tgt))
    return vocab_src, vocab_tgt
```

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æ•°æ®è¢«é€‚å½“åœ°é¢„å¤„ç†å’Œåˆ†è¯ï¼Œå¹¶æ„å»ºäº†é«˜è´¨é‡çš„è¯æ±‡è¡¨ã€‚è¿™äº›é¢„å¤„ç†æ­¥éª¤æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚



### 4.4 **æ‰¹å¤„ç†ä¸æ©ç **

#### 4.4.1 æ‰¹å¤„ç†

**æ‰¹å¤„ç†ï¼ˆ*Batch*ï¼‰**æ˜¯å°†æ•°æ®åˆ’åˆ†ä¸ºå°å—è¿›è¡Œå¤„ç†çš„ä¸€ç§æŠ€æœ¯ã€‚åœ¨Transformeræ¨¡å‹ä¸­ï¼Œæ‰¹å¤„ç†æœ‰ç€é‡è¦ä½œç”¨ï¼š

> * **æé«˜è®­ç»ƒæ•ˆç‡**ï¼š
>
>   - å°†æ•°æ®åˆ’åˆ†ä¸ºå°æ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æºï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼›
>
>   - å‡å°‘æ¨¡å‹è®­ç»ƒæ—¶æ‰€éœ€çš„å†…å­˜ç©ºé—´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡çš„æ•°æ®ã€‚
>
> * **åŠ é€Ÿè®¡ç®—**ï¼š
>
>   - æ‰¹å¤„ç†å¯ä»¥åˆ©ç”¨çŸ©é˜µè¿ç®—çš„å¹¶è¡Œæ€§ï¼ŒåŠ é€Ÿæ¨¡å‹çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ï¼›
>
>   - é€šè¿‡åŒæ—¶å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œå¯ä»¥æé«˜GPUçš„åˆ©ç”¨ç‡ï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚
>
> * **ç¨³å®šä¼˜åŒ–**ï¼š
>
>   - æ‰¹å¤„ç†å¯ä»¥å‡å°‘æ¯ä¸ªå‚æ•°æ›´æ–°çš„å™ªå£°ï¼Œä½¿å¾—ä¼˜åŒ–æ›´åŠ ç¨³å®šï¼›
>
>   - é€šè¿‡è®¡ç®—æ¯ä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦å¹³å‡å€¼è€Œä¸æ˜¯å•ä¸ªæ ·æœ¬çš„æ¢¯åº¦ï¼Œå¯ä»¥å‡å°‘æ¢¯åº¦çš„æ–¹å·®ï¼Œä»è€Œæ›´åŠ å¹³æ»‘åœ°æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

å®šä¹‰ ``Batch`` ç±»ï¼š

```python
class Batch:
    """Object for holding a batch of data with mask during training."""

    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>
        self.src = src 	
        # srcçš„å¤§å°æ˜¯[batch_size, seq_len]
        self.src_mask = (src != pad).unsqueeze(-2)
        # src_maskçš„å¤§å°æ˜¯[batch_size, 1, seq_len]ï¼Œmuskè¿ç®—æ—¶è‡ªåŠ¨æ‰©å¼ 
        # [batch_size, seq_len, seq_len]  
        # src_musk(i,j,k)è¡¨ç¤ºbatchç¬¬iä¸ªå¥å­çš„ç¬¬jä¸ªè¯å’Œç¬¬kä¸ªè¯çš„åˆ†æ•°æ˜¯å¦æ¸…é›¶ã€‚
        if tgt is not None:
            self.tgt = tgt[:, :-1]	#å»é™¤ç»“æŸç¬¦å·<end>
            self.tgt_y = tgt[:, 1:]	#å»é™¤å¼€å§‹ç¬¦å·<start>
            self.tgt_mask = self.make_std_mask(self.tgt, pad)	
            # tgt_maskçš„å¤§å°æ˜¯[batch_size, 1, seq_len]
            self.ntokens = (self.tgt_y != pad).data.sum()
            # ç»Ÿè®¡épadçš„è¯æ±‡æ•°

```

#### 4.4.2 æ©ç 

**æ©ç ï¼ˆ*Mask*ï¼‰**ç”¨äºåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­éšè—å¡«å……æ ‡è®°å’Œæœªæ¥è¯æ ‡è®°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶èƒ½å¤Ÿæ­£ç¡®åœ°è€ƒè™‘åˆ°è¿™äº›æ ‡è®°çš„å½±å“ï¼š

> * **é®è”½å¡«å……æ ‡è®°**ï¼š
>
>   - å¡«å……æ ‡è®°ï¼ˆ*padding token*ï¼‰é€šå¸¸ä¸åŒ…å«å®é™…çš„ä¿¡æ¯ï¼Œåªæ˜¯ä¸ºäº†ä½¿è¾“å…¥åºåˆ—å¯¹é½åˆ°ç›¸åŒé•¿åº¦ã€‚å› æ­¤ï¼Œåœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œéœ€è¦å°†å¡«å……æ ‡è®°å¯¹åº”çš„æ³¨æ„åŠ›åˆ†æ•°ç½®ä¸ºè´Ÿæ— ç©·å¤§ï¼Œä»è€Œåœ¨softmaxè®¡ç®—ä¸­è¢«å¿½ç•¥ï¼›
>
>   - é€šè¿‡æ©ç ï¼Œå¯ä»¥é¿å…æ¨¡å‹åœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶è€ƒè™‘åˆ°å¡«å……æ ‡è®°ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚
>
> * **é®è”½æœªæ¥è¯æ ‡è®°**ï¼š
>
>   - åœ¨è§£ç å™¨ç«¯ï¼Œä¸ºäº†ç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶åªä½¿ç”¨å…¶ä¹‹å‰çš„è¯ï¼Œéœ€è¦å°†å½“å‰è¯ä¹‹åçš„è¯åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­è¿›è¡Œæ©ç ã€‚è¿™æ ·å¯ä»¥é¿å…æ¨¡å‹åœ¨ç”Ÿæˆè¯æ—¶è·å–æœªæ¥è¯çš„ä¿¡æ¯ï¼Œä¿è¯äº†æ¨¡å‹çš„è‡ªå›å½’æ€§è´¨ï¼›
>
>   - é€šè¿‡æ©ç ï¼Œå¯ä»¥ç¡®ä¿æ¨¡å‹æŒ‰é¡ºåºç”Ÿæˆåºåˆ—ï¼Œé¿å…äº†ä¿¡æ¯æ³„éœ²å’Œé”™è¯¯ç”Ÿæˆçš„é—®é¢˜ã€‚

 ``Batch`` ç±»ä¸­ ``make_std_mask``æ–¹æ³•ï¼š

```python
    @staticmethod	#é™æ€æ–¹æ³•ï¼Œä¸éœ€è¦å®ä¾‹åŒ–å°±å¯ä»¥è°ƒç”¨ï¼Œä¸éœ€è¦selfå‚æ•°
    def make_std_mask(tgt, pad):
        "Create a mask to hide padding and future words."
        # å¦‚æœtgt=padï¼Œç›¸å½“äºå ä½ç¬¦ï¼Œæˆ‘ä»¬å¼ºåˆ¶è¦æ±‚æ‰€æœ‰åˆ†é…ç»™å ä½ç¬¦çš„attentionåˆ†æ•°æ˜¯-1e9
        # æ‰€ä»¥æˆ‘ä»¬ç›´æ¥musk
        tgt_mask = (tgt != pad).unsqueeze(-2)
        # å»æ‰å ä½ç¬¦
        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(
            tgt_mask.data
        )
        # å»æ‰è¿™ä¸ªè¯æ±‡ä¹‹åçš„è¯æ±‡
        # &è¿ç®—è¡¨ç¤ºæ»¡è¶³ä¸Šè¿°ä»»ä½•ä¸€ä¸ªæ¡ä»¶attentionå°±åº”è¯¥è¢«æ¸…é›¶
        
        return tgt_mask
```

å¯¹*batch*ä¸­æ•°æ®åšè¿›ä¸€æ­¥å¤„ç†ï¼Œè½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„å¼ é‡æ ¼å¼ï¼Œå¹¶è¿›è¡Œå¡«å……æ“ä½œ

* å®šä¹‰

  ```python
  def collate_batch(      
      batch,             
      src_pipeline,       # æºè¯­è¨€ç®¡é“å‡½æ•°
      tgt_pipeline,       # ç›®æ ‡è¯­è¨€ç®¡é“å‡½æ•°
      src_vocab,          # æºè¯­è¨€è¯æ±‡è¡¨
      tgt_vocab,			# ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨
      device,             # è®¾å¤‡
      max_padding=128,    # æœ€å¤§å¡«å……é•¿åº¦
      pad_id=2,           # å¡«å……æ ‡è®°
  ):
  ```

* æ·»åŠ æ ‡è®°

  ```python
      bs_id = torch.tensor([0], device=device)  # <s> token id èµ·å§‹æ ‡è®°
      eos_id = torch.tensor([1], device=device)  # </s> token id ç»“æŸæ ‡è®°
  ```

* å¤„ç†æºè¯­è¨€ï¼ˆç›®æ ‡è¯­è¨€ç±»ä¼¼ï¼‰

  ```python
  	for (_src, _tgt) in batch:
          processed_src = torch.cat(
              [
                  bs_id,      # <s> token id
                  torch.tensor(
                      src_vocab(src_pipeline(_src)),
                      dtype=torch.int64,
                      device=device,
                  ),
                  eos_id,     # </s> token id
              ],
              0,
          )
  ```

  > ä½¿ç”¨PyTorchçš„`torch.cat`å‡½æ•°ï¼Œå°†å¤šä¸ªå¼ é‡æŒ‰æŒ‡å®šç»´åº¦æ‹¼æ¥èµ·æ¥ï¼Œç”Ÿæˆå¤„ç†åçš„æºè¯­è¨€å¼ é‡ï¼›æ·»åŠ èµ·å§‹å’Œç»“æŸæ ‡è®°ï¼Œä»¥æç¤ºæ¨¡å‹å¥å­çš„å¼€å§‹å’Œç»“æŸï¼›å¹¶ä¸”æŒ‡å®šäº†æ•°æ®ç±»å‹ï¼ˆ*dtype*ï¼‰å’Œå­˜å‚¨è®¾å¤‡ï¼ˆ*device*ï¼‰ã€‚

* æºè¯­è¨€å¡«å……ï¼ˆç›®æ ‡è¯­è¨€ç±»ä¼¼ï¼‰

  ```python
      src_list.append(
          # warning - overwrites values for negative values of padding - len
          pad(
              processed_src,
              (
                  0,
                  max_padding - len(processed_src),
              ),
              value=pad_id,
          )
      )
  ```

* å †å å¼ é‡å¹¶è¿”å›

  ```python
      # åˆ—è¡¨ä¸­çš„æ‰€æœ‰å¼ é‡å †å ä¸ºä¸€ä¸ªæ‰¹å¤„ç†å¼ é‡ï¼Œç¡®ä¿æ‰€æœ‰å¥å­çš„é•¿åº¦ä¸€è‡´
      src = torch.stack(src_list)
      tgt = torch.stack(tgt_list)
      return (src, tgt)
  ```

  

### 4.5 ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦

è®ºæ–‡ä½¿ç”¨ *Adam* ä¼˜åŒ–å™¨ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼Œé€‚åº”ä¸åŒå‚æ•°çš„æ›´æ–°éœ€æ±‚ã€‚å–å‚æ•° $\beta_1=0.9$, $\beta_2=0.98$, $\epsilon=10^{-9}$ï¼Œæ¥æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå…¬å¼ä¸ºï¼š 
$$
lrate=d_{model}^{âˆ’0.5}â‹…minâ¡(step\_num^{âˆ’0.5},step\_numâ‹…warmup\_steps^{âˆ’1.5})
$$
è¿™ç›¸å½“äºåœ¨ç¬¬ä¸€ä¸ª *warmup_steps* è®­ç»ƒæ­¥éª¤ä¸­çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œæ­¤åæŒ‰æ­¥éª¤æ•°çš„åå¹³æ–¹æ ¹æ¯”ä¾‹å‡å°‘ã€‚è®­ç»ƒä¸­ä½¿ç”¨ *warmup_steps = 4000*.

è¿™æ ·é€‰å–ç¡®ä¿äº†åœ¨è®­ç»ƒåˆæœŸæ¨¡å‹èƒ½å¿«é€Ÿè°ƒæ•´å‚æ•°ä»¥é€‚åº”è®­ç»ƒæ•°æ®ï¼ŒåŒæ—¶åœ¨è®­ç»ƒåæœŸé€šè¿‡é™ä½å­¦ä¹ ç‡ä½¿æ¨¡å‹èƒ½å¤Ÿç¨³å®šæ”¶æ•›ï¼Œæœ€ç»ˆè¾¾åˆ°è¾ƒå¥½çš„æ€§èƒ½ã€‚

ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
def rate(step, model_size, factor, warmup):
    """
    we have to default the step to 1 for LambdaLR function
    to avoid zero raising to negative power.
    """
    if step == 0:
        step = 1
    return factor * (
        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))
    ) 
```

> ``rate``éš``step``ã€``warmup``ä»¥åŠ``model_size``å˜åŒ–è¶‹åŠ¿ï¼š![lrate](D:/Desktop/assets/lrate.png)



### 4.6 æ­£åˆ™åŒ–ä¸æŸå¤±å‡½æ•°

#### 4.6.1 **äº¤å‰ç†µæŸå¤±å‡½æ•°**

è®ºæ–‡ä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œå°¤å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„åºåˆ—ç”Ÿæˆä»»åŠ¡å¦‚æœºå™¨ç¿»è¯‘ä¸­ã€‚å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥ï¼Œæ¨¡å‹ç”Ÿæˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒè¡¨ç¤ºç”Ÿæˆæ¯ä¸ªè¯çš„æ¦‚ç‡ã€‚äº¤å‰ç†µæŸå¤±è¡¡é‡äº†æ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚

å…¬å¼è¡¨ç¤ºä¸ºï¼š
$$
\text{Cross-Entropy Loss} = - \sum_{i=1}^{N} y_i \log(p_i)
$$
å…¶ä¸­ï¼š

- $$y_i$$ æ˜¯çœŸå®æ ‡ç­¾çš„ç‹¬çƒ­ç¼–ç ï¼ˆ*one-hot encoding*ï¼‰ã€‚
- $$p_i$$ æ˜¯æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒã€‚

å¯¹äºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œå¹¶å¯¹æ‰€æœ‰æ—¶é—´æ­¥çš„æŸå¤±å–å¹³å‡å€¼ã€‚

ä»£ç å®ç°ï¼š

```python
import torch
import torch.nn as nn

# å®šä¹‰æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)

# ç¤ºä¾‹ï¼šè®¡ç®—æŸå¤±
def compute_loss(predictions, targets):
    """
    è®¡ç®—äº¤å‰ç†µæŸå¤±
    :param predictions: æ¨¡å‹çš„é¢„æµ‹ç»“æœ, å½¢çŠ¶ä¸º (batch_size, seq_len, vocab_size)
    :param targets: çœŸå®æ ‡ç­¾, å½¢çŠ¶ä¸º (batch_size, seq_len)
    :return: æŸå¤±å€¼
    """
    # å°†é¢„æµ‹ç»“æœè¿›è¡Œé‡å¡‘ï¼Œä½¿å…¶ç¬¦åˆäº¤å‰ç†µæŸå¤±å‡½æ•°çš„è¾“å…¥è¦æ±‚
    predictions = predictions.view(-1, predictions.size(-1))  # (batch_size * seq_len, vocab_size)
    targets = targets.view(-1)  # (batch_size * seq_len)
    
    # è®¡ç®—æŸå¤±
    loss = criterion(predictions, targets)
    return loss
```

> åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€æ­¥éƒ½è¦è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æŸå¤±ï¼Œç„¶åé€šè¿‡åå‘ä¼ æ’­ç®—æ³•æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚

äº¤å‰ç†µæŸå¤±å‡½æ•°çš„ä¼˜è¶Šæ€§åœ¨äºï¼š

1. **ç®€æ´æ˜äº†ï¼š**å…¶å…¬å¼ç®€å•ï¼Œå®¹æ˜“å®ç°ã€‚
2. **ç¨³å®šæ€§ï¼š**åœ¨å¤„ç†æ¦‚ç‡åˆ†å¸ƒæ—¶è¡¨ç°ç¨³å®šï¼Œèƒ½æœ‰æ•ˆé¿å…æ•°å€¼ä¸ç¨³å®šçš„é—®é¢˜ã€‚
3. **å¹¿æ³›åº”ç”¨ï¼š**åœ¨åˆ†ç±»å’Œåºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¢«å¹¿æ³›åº”ç”¨äºå„ç±»è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚



#### 4.6.2 **æ­£åˆ™åŒ–**

åœ¨è®ºæ–‡çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†é¢å¤–çš„æ­£åˆ™åŒ–ï¼ˆ*Regularization*ï¼‰æ¥å¢å¼ºæ¨¡å‹çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰ï¼Œé€šè¿‡æ”¹å˜ç›®æ ‡æ ‡ç­¾çš„åˆ†å¸ƒï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªè¯çš„é¢„æµ‹æ›´åŠ ä¸ç¡®å®šï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

* **æ ‡ç­¾å¹³æ»‘ï¼ˆ*Label Smoothing*ï¼‰**

æ ‡ç­¾å¹³æ»‘æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨é¿å…æ¨¡å‹è¿‡äºè‡ªä¿¡åœ°é¢„æµ‹ä¸€ä¸ªæ ‡ç­¾ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ä¸­ï¼Œç›®æ ‡æ ‡ç­¾æ˜¯ä¸€ä¸ªç‹¬çƒ­ç¼–ç ï¼ˆ*one-hot encoding*ï¼‰ï¼Œå³æ­£ç¡®æ ‡ç­¾çš„æ¦‚ç‡ä¸º1ï¼Œå…¶ä»–æ ‡ç­¾çš„æ¦‚ç‡ä¸º0ã€‚è€Œæ ‡ç­¾å¹³æ»‘åˆ™å°†ç›®æ ‡æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œä½¿å¾—æ­£ç¡®æ ‡ç­¾çš„æ¦‚ç‡ç¨å°äº1ï¼Œå…¶ä»–æ ‡ç­¾çš„æ¦‚ç‡ç¨å¤§äº0ã€‚

çœŸå®æ ‡ç­¾åˆ†å¸ƒ $P_{smooth}$ è¢«å®šä¹‰ï¼š
$$
P_{smooth}(y_i) = (1 - \epsilon) \cdot P_{one-hot}(y_i) + \epsilon / K
$$
å…¶ä¸­ï¼š

- $\epsilon$ ï¼šæ ‡ç­¾å¹³æ»‘å‚æ•°
- $K$ ï¼šç±»åˆ«æ•°
- $P_{one-hot}$ ï¼šone-hotç¼–ç çš„çœŸå®æ ‡ç­¾åˆ†å¸ƒã€‚

æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†æ ‡ç­¾å¹³æ»‘å€¼ $\epsilon_{ls} = 0.1$ã€‚è¿™ç§æ–¹æ³•è™½ç„¶ä¼šé™ä½æ¨¡å‹çš„å›°æƒ‘åº¦ï¼ˆ*perplexity*ï¼‰ï¼Œå› ä¸ºæ¨¡å‹ä¼šå˜å¾—æ›´åŠ ä¸ç¡®å®šï¼Œä½†å¯ä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œ BLEU å¾—åˆ†ã€‚

å…·ä½“å®ç°æ ‡ç­¾å¹³æ»‘çš„æ–¹æ³•æ˜¯é€šè¿‡ *Kullback-Leibler* æ•£åº¦æŸå¤±ï¼ˆ*KLDivLoss*ï¼‰ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ã€‚

ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
class LabelSmoothing(nn.Module):
    "Implement label smoothing."

    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(reduction="sum")  # æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨ Kullback-Leibler æ•£åº¦ä½œä¸ºæŸå¤±å‡½æ•°
        self.padding_idx = padding_idx      # å¡«å……æ ‡è®°çš„ç´¢å¼•ï¼Œç”¨äºåœ¨è®¡ç®—æŸå¤±æ—¶å¿½ç•¥å¡«å……éƒ¨åˆ†
        self.confidence = 1.0 - smoothing   # çœŸå®æ ‡ç­¾çš„ç½®ä¿¡åº¦
        self.smoothing = smoothing          # å¹³æ»‘å€¼
        self.size = size                    # æ ‡ç­¾çš„å¤§å°ï¼ˆç»´åº¦ï¼‰
        self.true_dist = None               # çœŸå®æ ‡ç­¾çš„åˆ†å¸ƒ

    # å‰å‘ä¼ æ’­å‡½æ•°
    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, true_dist.clone().detach())
```

> åœ¨ ``LabelSmoothing`` ç±»ä¸­å®šä¹‰æ–¹æ³• ``forward(self, x, target)`` ï¼Œè´Ÿè´£è®¡ç®—å¸¦æœ‰æ ‡ç­¾å¹³æ»‘çš„æŸå¤±å€¼ï¼š
>
> * è¿›è¡Œå°ºå¯¸æ£€æŸ¥
> * åˆ›å»ºå¹¶åˆå§‹åŒ–æ ‡ç­¾åˆ†å¸ƒ
> * å¡«å……æ­£ç¡®æ ‡ç­¾çš„ç½®ä¿¡åº¦
> * å¤„ç†å¡«å……æ ‡è®°
> * ä¿å­˜çœŸå®æ ‡ç­¾åˆ†å¸ƒ
> * è®¡ç®—æŸå¤±

æ ‡ç­¾å¹³æ»‘çš„å¥½å¤„åœ¨äºï¼š

1. **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šé€šè¿‡å‡å°‘æ¨¡å‹å¯¹å•ä¸ªæ ‡ç­¾çš„ç½®ä¿¡åº¦ï¼Œæ ‡ç­¾å¹³æ»‘èƒ½æœ‰æ•ˆé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼›
2. **æé«˜æ³›åŒ–æ€§èƒ½**ï¼šæ ‡ç­¾å¹³æ»‘ä½¿æ¨¡å‹åœ¨æœªçŸ¥æ•°æ®ä¸Šçš„è¡¨ç°æ›´åŠ ç¨³å¥ï¼Œæå‡æ³›åŒ–æ€§èƒ½ï¼›
3. **ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹**ï¼šé€šè¿‡ä½¿ç”¨KLæ•£åº¦æŸå¤±ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æ›´åŠ å¹³æ»‘å’Œç¨³å®šã€‚



#### 4.6.3 æ•£åº¦æŸå¤±å‡½æ•°

*Kullback-Leibler* æ•£åº¦æŸå¤±ï¼ˆ*KLDivLoss*ï¼‰å‡½æ•°æ˜¯ç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚çš„ä¸€ç§æŸå¤±å‡½æ•°ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸ç”¨äºè¡¡é‡æ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚å…¶åŸç†æ˜¯é€šè¿‡è®¡ç®—ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„ *KLæ•£åº¦* æ¥è¡¡é‡å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼æ€§æˆ–å·®å¼‚æ€§ã€‚

*KL*æ•£åº¦æ˜¯ä¿¡æ¯è®ºä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼Œç”¨äºè¡¡é‡ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒç›¸å¯¹äºå¦ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„â€œè·ç¦»â€ã€‚å¯¹äºä¸¤ä¸ªç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ*P*å’Œ*Q*ï¼Œ*KL*æ•£åº¦çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
$$
D_{KL}(P || Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}
$$


å…¶ä¸­ï¼Œ$P(i)$ å’Œ $Q(i)$ åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒåœ¨ç¬¬ $i$ ä¸ªäº‹ä»¶ä¸Šçš„æ¦‚ç‡ã€‚

* ä¸¤ç§æŸå¤±å‡½æ•°å¯¹æ¯”

> ç›¸å¯¹äºäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œ*KLDivLoss* çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°æ˜¯è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„â€œè·ç¦»â€ï¼Œè€Œ*KLDivLoss*æ˜¯è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„â€œå·®å¼‚æ€§â€ã€‚å…·ä½“æ¥è¯´ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°å…³æ³¨çš„æ˜¯é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç†µï¼Œè€Œ *KLDivLoss* å…³æ³¨çš„æ˜¯é¢„æµ‹åˆ†å¸ƒç›¸å¯¹äºçœŸå®åˆ†å¸ƒçš„KLæ•£åº¦ã€‚

* *KLDivLoss*çš„ä¼˜è¶Šæ€§

> KLDivLossç›¸å¯¹äºäº¤å‰ç†µæŸå¤±å‡½æ•°çš„ä¼˜è¶Šæ€§åœ¨äºï¼Œå®ƒæ›´åŠ çµæ´»ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒåˆ†å¸ƒä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸ä»…é™äºé¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„åŒ¹é…ã€‚å› æ­¤ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹è¾“å‡ºçš„æ˜¯æ¦‚ç‡åˆ†å¸ƒè€Œä¸æ˜¯ç±»åˆ«æ ‡ç­¾æ—¶ï¼ŒKLDivLosså¯ä»¥æ›´å¥½åœ°åæ˜ æ¨¡å‹çš„æ€§èƒ½ã€‚



### 4.7 è®­ç»ƒè¿‡ç¨‹å’Œç»†èŠ‚

è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹çš„å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°ä»¥åŠå­¦ä¹ ç‡è°ƒåº¦ç­‰æ­¥éª¤ã€‚

#### 4.7.1 **è®­ç»ƒçŠ¶æ€è·Ÿè¸ª**

é¦–å…ˆï¼Œå®šä¹‰äº†ä¸€ä¸ª`TrainState`ç±»æ¥è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¸€äº›å…³é”®æŒ‡æ ‡ï¼š

```python
class TrainState:
    """Track number of steps, examples, and tokens processed"""
    step: int = 0  			# å½“å‰epochä¸­çš„æ­¥æ•°
    accum_step: int = 0  	# ç´¯è®¡çš„æ¢¯åº¦ç§¯ç´¯æ­¥æ•°
    samples: int = 0  		# ä½¿ç”¨çš„æ ·æœ¬æ€»æ•°
    tokens: int = 0  		# å¤„ç†çš„tokensæ€»æ•°
```



#### 4.7.2 **è®­ç»ƒå‘¨æœŸï¼ˆ*epochï¼‰***

å®šä¹‰`run_epoch`å‡½æ•°è¿è¡Œæ¯ä¸ªè®­ç»ƒå‘¨æœŸï¼ˆ*epoch*ï¼‰ï¼ŒåŒ…æ‹¬å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š

```python
def run_epoch(
    data_iter,  	# è¿­ä»£å™¨,æ¯æ¬¡è¿­ä»£è¿”å›ä¸€ä¸ªbatch,è¿™ä¸ªbatchæ˜¯ä¸€ä¸ªBatchå¯¹è±¡,åŒ…å«src,tgt,pad
    model,      	# å¾…è®­ç»ƒæ¨¡å‹
    loss_compute,   # æŸå¤±è®¡ç®—æ–¹æ³•
    optimizer,      # ä¼˜åŒ–å™¨ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°
    scheduler,      # å­¦ä¹ ç‡è°ƒåº¦å™¨
    mode="train",   # è®­ç»ƒæ¨¡å¼ or â€œtrain + logâ€ï¼ˆè®­ç»ƒæ¨¡å¼å¹¶è®°å½•æ—¥å¿—ï¼‰
    accum_iter=1,   # ç´¯è®¡è¿­ä»£æ¬¡æ•°
    train_state=TrainState(),
):
    """Train a single epoch"""
```

* åˆå§‹åŒ–å‚æ•°

  ```python
      start = time.time() # è®°å½•å¼€å§‹æ—¶é—´
      total_tokens = 0
      total_loss = 0
      tokens = 0
      n_accum = 0
  ```

* å‰å‘ä¼ æ’­ä¸æŸå¤±è®¡ç®—

  ```python
      for i, batch in enumerate(data_iter):
          out = model.forward(
              batch.src, batch.tgt, batch.src_mask, batch.tgt_mask
          )
          loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)
          # loss_node = loss_node / accum_iter
  ```

* è®­ç»ƒçŠ¶æ€æ›´æ–°

  ```python
          if mode == "train" or mode == "train+log":
              loss_node.backward()    # è°ƒç”¨ backward æ–¹æ³•è®¡ç®—æŸå¤±å…³äºå‚æ•°çš„æ¢¯åº¦
              train_state.step += 1   # è®°å½•æ­¥æ•°
              train_state.samples += batch.src.shape[0]   # è®°å½•æ ·æœ¬æ•°
              train_state.tokens += batch.ntokens  # è®°å½•tokenæ•°
              if i % accum_iter == 0:
                  optimizer.step()    # æ›´æ–°å‚æ•°
                  optimizer.zero_grad(set_to_none=True)   # æ¢¯åº¦æ¸…é›¶
                  n_accum += 1
                  train_state.accum_step += 1
              scheduler.step()    # æ›´æ–°å­¦ä¹ ç‡
  
          total_loss += loss
          total_tokens += batch.ntokens
          tokens += batch.ntokens
  ```

* æ‰“å°æ—¥å¿—ã€è®­ç»ƒä¿¡æ¯

  ```python
          if i % 40 == 1 and (mode == "train" or mode == "train+log"):
              lr = optimizer.param_groups[0]["lr"]
              elapsed = time.time() - start
              print(
                  (
                      "Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f "
                      + "| Tokens / Sec: %7.1f | Learning Rate: %6.1e"
                  )
                  % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)
              )   # æ‰“å°æ—¥å¿—
              start = time.time()
              tokens = 0
          del loss
          del loss_node
      return total_loss / total_tokens, train_state
  ```

  > æ¯è¿è¡Œä¸€å®š ``step`` ï¼Œä¾¿ä¼šæ‰“å°æ—¥å¿—ä¿¡æ¯ï¼Œå¦‚ï¼š
  >
  > ![image-20240610103819289](C:\Users\Lucky Lee\AppData\Roaming\Typora\typora-user-images\image-20240610103819289.png)



#### 4.7.3 **è®­ç»ƒå·¥ä½œå‡½æ•°**

``train_worker`` å‡½æ•°è´Ÿè´£åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå®šä¹‰å¦‚ä¸‹ï¼š

```python
def train_worker(
    gpu,
    ngpus_per_node,
    vocab_src,      # æºè¯­è¨€è¯æ±‡è¡¨
    vocab_tgt,      # ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨
    spacy_de,
    spacy_en,
    config,         # é…ç½®å­—å…¸ï¼ŒåŒ…å«è®­ç»ƒå‚æ•°
    is_distributed=False,   # æ˜¯å¦åˆ†å¸ƒå¼è®­ç»ƒ
):
```

> å‡½æ•°åŒ…å«ï¼š
>
> * åˆå§‹åŒ–è®¾ç½®ï¼š
>   * è®¾ç½®GPUè®¾å¤‡ã€‚
>   * å®šä¹‰æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬ç¼–ç å™¨ã€è§£ç å™¨å’Œç”Ÿæˆå™¨ã€‚
>   * å®šä¹‰æŸå¤±å‡½æ•°ï¼Œè¿™é‡Œä½¿ç”¨äº†æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰æŠ€æœ¯ã€‚
>   * åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®åŠ è½½å™¨ã€‚
>   * å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆAdamä¼˜åŒ–å™¨ï¼‰å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆLambdaLRè°ƒåº¦å™¨ï¼‰ã€‚
> * **è®­ç»ƒå’ŒéªŒè¯å¾ªç¯**ï¼š
>   - å¾ªç¯éå†æ¯ä¸ªepochï¼Œè¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚
>   - åœ¨æ¯ä¸ªepochä¸­ï¼Œé€šè¿‡è°ƒç”¨`run_epoch`å‡½æ•°æ¥æ‰§è¡Œä¸€ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯è¿‡ç¨‹ã€‚
>   - è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨æ¢¯åº¦ç´¯ç§¯çš„æ–¹å¼è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå¹¶é€šè¿‡å­¦ä¹ ç‡è°ƒåº¦å™¨åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ã€‚
>   - æ¯ä¸ªepochç»“æŸåï¼Œä¿å­˜æ¨¡å‹çš„çŠ¶æ€ã€‚
> * **GPUåˆ©ç”¨ç‡å’Œæ¨¡å‹çŠ¶æ€ä¿å­˜**ï¼š
>   * åœ¨æ¯ä¸ªepochç»“æŸåï¼Œæ˜¾ç¤ºGPUåˆ©ç”¨ç‡ã€‚
>   * åœ¨ä¸»è¿›ç¨‹ä¸­ä¿å­˜æ¨¡å‹çš„çŠ¶æ€ã€‚

åŒæ—¶å®šä¹‰äº†åˆ†å¸ƒå¼è®­ç»ƒå‡½æ•°``train_distributed_model``ã€‚



## äº”ã€å®éªŒç»“æœä¸åˆ†æ

### 5.1 è®ºæ–‡ç»“æœ

è®ºæ–‡åœ¨WMT 2014è‹±è¯­åˆ°å¾·è¯­å’Œè‹±è¯­åˆ°æ³•è¯­çš„ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTransformeræ¨¡å‹åœ¨BLEUè¯„åˆ†ä¸Šå‡å–å¾—äº†æ˜¾è‘—ä¼˜äºå¯¹æ¯”æ¨¡å‹çš„ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è‹±è¯­åˆ°å¾·è¯­çš„ç¿»è¯‘ä»»åŠ¡ä¸Šï¼ŒTransformeræ¨¡å‹è¾¾åˆ°äº†28.4çš„BLEUè¯„åˆ†ï¼Œæ¯”å½“æ—¶çš„æœ€ä½³ç»“æœæé«˜äº†è¶…è¿‡2ä¸ªBLEUç‚¹ã€‚åœ¨è‹±è¯­åˆ°æ³•è¯­çš„ç¿»è¯‘ä»»åŠ¡ä¸Šï¼ŒTransformeræ¨¡å‹åœ¨è®­ç»ƒäº†3.5å¤©åï¼Œè¾¾åˆ°äº†41.8çš„BLEUè¯„åˆ†ï¼Œåˆ·æ–°äº†å•æ¨¡å‹çš„æœ€ä½³æˆç»©ã€‚



### 5.2 å¯¹æ¯”æ¨¡å‹

åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸï¼ŒRNNåŠå…¶å˜ç§å¦‚LSTMå’ŒGRUä¸€ç›´æ˜¯ä¸»æµæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šè¿‡å¾ªç¯ç»“æ„æ•æ‰åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š

* æ— æ³•æœ‰æ•ˆå¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»
* éš¾ä»¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†åŸºäºCNNçš„æ¨¡å‹ï¼Œå¦‚ByteNetå’ŒFairSeqï¼Œè¿™äº›æ¨¡å‹é€šè¿‡å·ç§¯æ“ä½œæ•æ‰å±€éƒ¨ç‰¹å¾ï¼Œä½†åŒæ ·å­˜åœ¨éš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»çš„é—®é¢˜ã€‚

Transformeræ¶æ„çš„æå‡ºï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å®ƒå®Œå…¨åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ‰€æœ‰ä½ç½®ä¹‹é—´å»ºç«‹ç›´æ¥çš„è”ç³»ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚åŒæ—¶ï¼Œç”±äºTransformerçš„è®¡ç®—è¿‡ç¨‹ä¸ä¾èµ–äºå‰ä¸€æ—¶åˆ»çš„è®¡ç®—ç»“æœï¼Œå› æ­¤å¯ä»¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚



### 5.3 å¤ç°ç»“æœ

#### 5.3.1 è®­ç»ƒç»“æœ

æœ¬æ¬¡å®éªŒä¸­ä½¿ç”¨ Multi30k æ•°æ®é›†å¤ç°äº†è®ºæ–‡ç»“æœï¼Œä»¥ä¸‹æ˜¯ç¬¬é›¶å‘¨æœŸï¼ˆEpoch 0ï¼‰å’Œæœ€åä¸€ä¸ªå‘¨æœŸï¼ˆEpoch 7ï¼‰çš„è®­ç»ƒæ—¥å¿—è¾“å‡ºï¼š

```python
# åŠ è½½æ•°æ®é›†
Vocabulary sizes:
8317
6384
```

```python
# ç¬¬é›¶å‘¨æœŸ
[GPU0] Epoch 0 Training ====
Epoch Step:      1 | Accumulation Step:   1 | Loss:   7.61 | Tokens / Sec:  2465.8 | Learning Rate: 5.4e-07
Epoch Step:     41 | Accumulation Step:   5 | Loss:   7.38 | Tokens / Sec:  3829.8 | Learning Rate: 1.1e-05
Epoch Step:     81 | Accumulation Step:   9 | Loss:   6.95 | Tokens / Sec:  4032.1 | Learning Rate: 2.2e-05
Epoch Step:    121 | Accumulation Step:  13 | Loss:   6.60 | Tokens / Sec:  3973.4 | Learning Rate: 3.3e-05
Epoch Step:    161 | Accumulation Step:  17 | Loss:   6.41 | Tokens / Sec:  3978.0 | Learning Rate: 4.4e-05
Epoch Step:    201 | Accumulation Step:  21 | Loss:   6.22 | Tokens / Sec:  3957.2 | Learning Rate: 5.4e-05
Epoch Step:    241 | Accumulation Step:  25 | Loss:   6.16 | Tokens / Sec:  4010.9 | Learning Rate: 6.5e-05
Epoch Step:    281 | Accumulation Step:  29 | Loss:   6.02 | Tokens / Sec:  3776.2 | Learning Rate: 7.6e-05
Epoch Step:    321 | Accumulation Step:  33 | Loss:   5.88 | Tokens / Sec:  3951.3 | Learning Rate: 8.7e-05
Epoch Step:    361 | Accumulation Step:  37 | Loss:   5.60 | Tokens / Sec:  4089.8 | Learning Rate: 9.7e-05
Epoch Step:    401 | Accumulation Step:  41 | Loss:   5.33 | Tokens / Sec:  4241.4 | Learning Rate: 1.1e-04
Epoch Step:    441 | Accumulation Step:  45 | Loss:   5.19 | Tokens / Sec:  3891.9 | Learning Rate: 1.2e-04
Epoch Step:    481 | Accumulation Step:  49 | Loss:   4.81 | Tokens / Sec:  3705.6 | Learning Rate: 1.3e-04
Epoch Step:    521 | Accumulation Step:  53 | Loss:   4.57 | Tokens / Sec:  4065.9 | Learning Rate: 1.4e-04
Epoch Step:    561 | Accumulation Step:  57 | Loss:   4.51 | Tokens / Sec:  4114.3 | Learning Rate: 1.5e-04
Epoch Step:    601 | Accumulation Step:  61 | Loss:   4.32 | Tokens / Sec:  4112.3 | Learning Rate: 1.6e-04
Epoch Step:    641 | Accumulation Step:  65 | Loss:   4.32 | Tokens / Sec:  4081.6 | Learning Rate: 1.7e-04
Epoch Step:    681 | Accumulation Step:  69 | Loss:   4.19 | Tokens / Sec:  4130.6 | Learning Rate: 1.8e-04
Epoch Step:    721 | Accumulation Step:  73 | Loss:   4.26 | Tokens / Sec:  4119.8 | Learning Rate: 1.9e-04
Epoch Step:    761 | Accumulation Step:  77 | Loss:   4.23 | Tokens / Sec:  4059.4 | Learning Rate: 2.0e-04
Epoch Step:    801 | Accumulation Step:  81 | Loss:   4.02 | Tokens / Sec:  4085.5 | Learning Rate: 2.2e-04
Epoch Step:    841 | Accumulation Step:  85 | Loss:   4.08 | Tokens / Sec:  4113.0 | Learning Rate: 2.3e-04
Epoch Step:    881 | Accumulation Step:  89 | Loss:   4.08 | Tokens / Sec:  4152.6 | Learning Rate: 2.4e-04
| ID | GPU | MEM |
------------------
|  0 | 11% | 29% |
[GPU0] Epoch 0 Validation ====
(tensor(3.9010, device='cuda:0'), <__main__.TrainState object at 0x14a750266070>)
```

```python
# ç¬¬ä¸ƒå‘¨æœŸï¼ˆæœ€åå‘¨æœŸï¼‰
[GPU0] Epoch 7 Training ====
Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.00 | Tokens / Sec:  3479.8 | Learning Rate: 5.5e-04
Epoch Step:     41 | Accumulation Step:   5 | Loss:   0.89 | Tokens / Sec:  4120.2 | Learning Rate: 5.5e-04
Epoch Step:     81 | Accumulation Step:   9 | Loss:   0.89 | Tokens / Sec:  4062.8 | Learning Rate: 5.5e-04
Epoch Step:    121 | Accumulation Step:  13 | Loss:   0.91 | Tokens / Sec:  4037.1 | Learning Rate: 5.5e-04
Epoch Step:    161 | Accumulation Step:  17 | Loss:   0.97 | Tokens / Sec:  3650.3 | Learning Rate: 5.5e-04
Epoch Step:    201 | Accumulation Step:  21 | Loss:   1.20 | Tokens / Sec:  3703.5 | Learning Rate: 5.5e-04
Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.05 | Tokens / Sec:  3798.1 | Learning Rate: 5.4e-04
Epoch Step:    281 | Accumulation Step:  29 | Loss:   0.99 | Tokens / Sec:  4196.5 | Learning Rate: 5.4e-04
Epoch Step:    321 | Accumulation Step:  33 | Loss:   0.97 | Tokens / Sec:  4178.9 | Learning Rate: 5.4e-04
Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.07 | Tokens / Sec:  4177.1 | Learning Rate: 5.4e-04
Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.00 | Tokens / Sec:  4085.0 | Learning Rate: 5.4e-04
Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.01 | Tokens / Sec:  3785.2 | Learning Rate: 5.4e-04
Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.13 | Tokens / Sec:  3722.8 | Learning Rate: 5.3e-04
Epoch Step:    521 | Accumulation Step:  53 | Loss:   0.97 | Tokens / Sec:  3926.1 | Learning Rate: 5.3e-04
Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.15 | Tokens / Sec:  4078.5 | Learning Rate: 5.3e-04
Epoch Step:    601 | Accumulation Step:  61 | Loss:   0.91 | Tokens / Sec:  4113.8 | Learning Rate: 5.3e-04
Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.04 | Tokens / Sec:  4229.8 | Learning Rate: 5.3e-04
Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.35 | Tokens / Sec:  4072.1 | Learning Rate: 5.3e-04
Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.01 | Tokens / Sec:  4147.8 | Learning Rate: 5.3e-04
Epoch Step:    761 | Accumulation Step:  77 | Loss:   0.82 | Tokens / Sec:  4159.1 | Learning Rate: 5.2e-04
Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.03 | Tokens / Sec:  4116.5 | Learning Rate: 5.2e-04
Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.11 | Tokens / Sec:  4164.7 | Learning Rate: 5.2e-04
Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.13 | Tokens / Sec:  4209.2 | Learning Rate: 5.2e-04
| ID | GPU | MEM |
------------------
|  0 | 33% | 33% |
[GPU0] Epoch 7 Validation ====
(tensor(1.4455, device='cuda:0'), <__main__.TrainState object at 0x14a750266070>)
```

å¯ä»¥ä»ç»“æœä¸­æ˜æ˜¾è§‚å¯Ÿåˆ°ï¼š

1. æŸå¤± *Loss* æœ‰æŒç»­é™ä½çš„è¶‹åŠ¿ï¼Œåˆå§‹ä¸º7.61ï¼Œåœ¨ *Epoch1*ä¸­é™åˆ°4.08ï¼Œåœ¨è®­ç»ƒç»“æŸé™è‡³1.13ï¼Œæ­£ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œè¡¨æ˜æ¨¡å‹åœ¨ *Epoch7*çš„è®­ç»ƒåèƒ½å¤Ÿæ›´å¥½åœ°æ‹ŸåˆéªŒè¯é›†ï¼›åŒæ—¶ *Loss* åœ¨ *Epoch7* ä¸­æ— è§„å¾‹æ³¢åŠ¨åœ¨ 1 é™„è¿‘ï¼Œè¯´æ˜æ¨¡å‹å·²ç»è¿›è¡Œå……åˆ†è®­ç»ƒï¼Œå¾—åˆ°æœ€ä¼˜ç»“æœã€‚
2. å­¦ä¹ ç‡ Learning Rate å…ˆå¢åå‡ï¼Œåœ¨ Epoch 3 Step 281 è¾¾åˆ°å³°å€¼ $8.1Ã—10^{-4}$ ï¼Œç¬¦åˆè®¾å®šï¼š

```python
[GPU0] Epoch 3 Training ====
......
Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.89 | Tokens / Sec:  4329.7 | Learning Rate: 8.0e-04
Epoch Step:    281 | Accumulation Step:  29 | Loss:   2.10 | Tokens / Sec:  4276.3 | Learning Rate: 8.1e-04
Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.82 | Tokens / Sec:  4233.8 | Learning Rate: 8.0e-04
Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.93 | Tokens / Sec:  4338.3 | Learning Rate: 8.0e-04
......
```

3. è®­ç»ƒè¿‡ç¨‹ä¸­GPUåˆ©ç”¨ç‡å‡ºç°äº†ä¸€å®šæ³¢åŠ¨ï¼Œä¾‹å¦‚ï¼Œä»ç¬¬ä¸€å‘¨æœŸåˆ°ç¬¬äºŒå‘¨æœŸï¼ŒGPUåˆ©ç”¨ç‡ä»11%ä¸Šå‡29%ï¼Œé€šå¸¸åœ¨ç¬¬ä¸‰å‘¨æœŸæ—¶ä¸Šå‡38%ï¼Œä¹‹ååˆé™è‡³10%ï¼Œæœ€ç»ˆåœ¨ç¬¬ä¸ƒå‘¨æœŸè¾¾åˆ°äº†33%ã€‚è¿™ç§æ³¢åŠ¨å¯èƒ½æ˜¯ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­ä¸åŒæ‰¹æ¬¡çš„æ•°æ®ç‰¹æ€§ä»¥åŠæ¨¡å‹å‚æ•°çš„å˜åŒ–å¼•èµ·çš„ã€‚GPUåˆ©ç”¨ç‡ä¿æŒåœ¨åˆç†èŒƒå›´å†…ï¼Œè¡¨æ˜æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—èµ„æºæ¶ˆè€—è¾ƒä¸ºåˆç†



ä»¥ä¸‹æ˜¯ä¸€äº›å®ä¾‹è¾“å…¥è¾“å‡ºï¼Œå±•ç°è®­ç»ƒç¿»è¯‘ç»“æœï¼š

```
# Example 1
Source Text (Input)        : <s> Ein Ã¤lterer Mann sitzt im Freien vor einem groÃŸen Banner mit der Aufschrift â€ <unk> <unk> <unk> <unk> â€œ auf einer Bank . </s>
Target Text (Ground Truth) : <s> An older man is sitting outside on a bench in front a large banner that says , " <unk> <unk> <unk> <unk> . " </s>
Model Output               : <s> An elderly man sits outside in front of a large banner that says " <unk> " <unk> " on a bench . </s>
```

```
# Example 2
Source Text (Input)        : <s> Eine Frau und ein Junge auf einer BÃ¼hne lachen mit einem Zelt im Hintergrund . </s>
Target Text (Ground Truth) : <s> A woman and a boy on stage laughing with a tent in the background . </s>
Model Output               : <s> A woman and a boy are laughing with a tent in the background . </s>
```

```
# Example 3
Source Text (Input)        : <s> Eine junge <unk> Ã¼berprÃ¼ft , ob sie neue <unk> von Kunden erhalten hat . </s>
Target Text (Ground Truth) : <s> A young clerk looking for new <unk> from <unk> . </s>
Model Output               : <s> A young female <unk> is checking out new customers . </s>
```

#### 5.3.2 å¯è§†åŒ–

æˆ‘ä»¬ç›´æ¥çœ‹ä¾‹å­ï¼Œé¢œè‰²æ·±æµ…è¡¨ç¤ºæ³¨æ„åŠ›å¤§å°ã€‚

![image-20240610231324774](.\assets\image-20240610231324774.png)

encodeéƒ¨åˆ†çš„å¾·è¯­self-attentionã€‚æ•´ä½“å›´ç»•ä¸»å¯¹è§’çº¿åˆ†å¸ƒã€‚è¯´æ˜è‡ªå·±è¿˜æ˜¯æœ€å…³æ³¨è‡ªå·±ã€‚

![image-20240610231506515](.\assets\image-20240610231506515.png)

è‹±è¯­å’Œå¾·è¯­çš„cross attentionï¼Œå¯ä»¥çœ‹åˆ°å„ä¸ªè¯­è¨€çš„è¯­åºå¤§ä½“éµå¾ªåŒæ ·çš„æ—¶åºã€‚æ•´ä½“å›´ç»•ä¸»å¯¹è§’çº¿åˆ†å¸ƒã€‚





## å…­ã€åŸè®ºæ–‡transformerå±€é™æ€§åŠä¼˜åŒ–æ‹“å±•

### 5.1 transformerå±€é™æ€§

è™½ç„¶Transformeræ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œä½†å®ƒä¹Ÿå­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¸‹é¢æ˜¯å…¶ä¸­ä¸€äº›ä¸»è¦çš„å±€é™æ€§ï¼š

1. **è®¡ç®—èµ„æºéœ€æ±‚é«˜ï¼š** Transformeræ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºæ¥è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚ç”±äºå…¶å¤æ‚çš„æ¨¡å‹ç»“æ„å’Œå¤§é‡çš„å‚æ•°ï¼Œéœ€è¦è¾ƒå¤§çš„æ¨¡å‹å®¹é‡å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚è¿™é™åˆ¶äº†Transformeræ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. **åºåˆ—é•¿åº¦é™åˆ¶ï¼š** Transformeræ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶å­˜åœ¨å›°éš¾ã€‚ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚æ€§ï¼Œéšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œæ¨¡å‹çš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚å‘ˆç°äºŒæ¬¡å¢åŠ ã€‚å› æ­¤ï¼Œå¯¹äºéå¸¸é•¿çš„åºåˆ—ï¼ˆä¾‹å¦‚é•¿æ–‡æ¡£æˆ–é•¿æ—¶é—´åºåˆ—ï¼‰ï¼ŒTransformerå¯èƒ½ä¼šé¢ä¸´å†…å­˜é™åˆ¶å’Œæ•ˆç‡é—®é¢˜ã€‚
3. **ç¼ºä¹é€æ­¥æ¨ç†èƒ½åŠ›ï¼š** Transformeræ¨¡å‹æ˜¯ä¸€æ¬¡æ€§å¯¹æ•´ä¸ªè¾“å…¥åºåˆ—è¿›è¡Œå¤„ç†çš„ï¼Œæ— æ³•åœ¨ç”Ÿæˆåºåˆ—æ—¶é€æ­¥æ¨ç†ã€‚è¿™æ„å‘³ç€å®ƒæ— æ³•æ¨¡æ‹Ÿäººç±»é€æ­¥æ€è€ƒå’Œç”Ÿæˆçš„è¿‡ç¨‹ã€‚åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œé€æ­¥ç”Ÿæˆçš„èƒ½åŠ›å¯èƒ½æ˜¯å¿…éœ€çš„ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ä¸­çš„è§£ç è¿‡ç¨‹ã€‚
4. **å¯¹è¾“å…¥é¡ºåºæ•æ„Ÿï¼š** Transformeræ¨¡å‹å¯¹è¾“å…¥åºåˆ—çš„é¡ºåºæ˜¯æ•æ„Ÿçš„ã€‚è™½ç„¶è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä½†å¯¹äºæ¶‰åŠä¸Šä¸‹æ–‡é¡ºåºçš„ä»»åŠ¡ï¼ˆå¦‚è¯­è¨€ç†è§£ï¼‰ï¼Œè¾“å…¥åºåˆ—çš„é¡ºåºå¯èƒ½å¯¹æ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿé‡è¦å½±å“ã€‚è¿™ä¹Ÿå¯¼è‡´Transformeræ¨¡å‹å¯¹äºå¤„ç†å…·æœ‰æ—¶é—´ä¾èµ–æ€§çš„ä»»åŠ¡ï¼ˆå¦‚è§†é¢‘å¤„ç†æˆ–è¿ç»­åŠ¨ä½œé¢„æµ‹ï¼‰ç›¸å¯¹å›°éš¾ã€‚
5. **æ•°æ®éœ€æ±‚é‡å¤§ï¼š** Transformeræ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µã€‚è¿™å¯¹äºæŸäº›é¢†åŸŸæˆ–ç‰¹å®šä»»åŠ¡å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè·å–å¤§è§„æ¨¡æ ‡è®°æ•°æ®å¯èƒ½æ˜¯å›°éš¾æˆ–æ˜‚è´µçš„ã€‚

### 5.2 transformer åœ¨ è®¡ç®—æœºè§†è§‰çš„åº”ç”¨

#### 5.2.1 VIT

Visual Transformerï¼ˆViTï¼‰æ˜¯ä¸€ç§åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸåº”ç”¨çš„æ¨¡å‹ï¼Œå®ƒå°†å›¾åƒåˆ†å‰²æˆå°å—ï¼Œå¹¶é€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹æ¯ä¸ªå°å—è¿›è¡Œç‰¹å¾æå–ã€‚è¿™äº›å°å—è¢«è§†ä¸ºç‹¬ç«‹çš„"token"ï¼Œç„¶åè¢«è¾“å…¥åˆ°Transformeræ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥è¢«çœ‹ä½œæ˜¯å°†æ•´ä¸ªå›¾åƒè½¬åŒ–ä¸ºä¸€ä¸ªå‘é‡è¡¨ç¤ºçš„è¿‡ç¨‹ï¼Œå…¶ä¸­æ¯ä¸ªå°å—éƒ½å¯¹åº”ä¸€ä¸ª"token"ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å¯¹ç‰¹å¾è¿›è¡Œçº¿æ€§è½¬æ¢ï¼Œå¾—åˆ°äº†æ¯ä¸ªå°å—çš„å‘é‡è¡¨ç¤ºã€‚

æ€»çš„æ¥è¯´ï¼ŒViTæ¨¡å‹å°†å›¾åƒåˆ†å‰²æˆå°åŒºåŸŸï¼Œç„¶åé€šè¿‡CNNæå–ç‰¹å¾ã€‚è¿™äº›åŒºåŸŸè¢«çœ‹ä½œæ˜¯ç‹¬ç«‹çš„"token"ï¼Œå¹¶é€šè¿‡Transformeræ¨¡å‹è¿›è¡Œå¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹å°†æ•´ä¸ªå›¾åƒè½¬åŒ–ä¸ºä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼Œå…¶ä¸­æ¯ä¸ªå°åŒºåŸŸå¯¹åº”ä¸€ä¸ª"token"ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ViTèƒ½å¤Ÿåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¿›è¡Œç«¯åˆ°ç«¯çš„å­¦ä¹ å’Œé¢„æµ‹ï¼Œå¹¶å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚

![image-20240529121152392](.\assets\image-20240529121152392.png)

#### 5.2.2 diffusion

åœ¨å½“å‰ç«çˆ†çš„æ–‡ç”Ÿå›¾æ¨¡å‹ä¸­ï¼Œvitä½œä¸ºæ ¸å¿ƒæ¨¡å—è¢«åµŒå…¥åˆ°Unetå½“ä¸­ï¼Œç”¨æ¥é¢„æµ‹diffusionçš„å™ªå£°ã€‚

![image-20240529131521414](.\assets\image-20240529131521414.png)

attentionæ¨¡å—çš„èŠ±è´¹æ—¶é—´æ˜¯æ¯”è¾ƒå¤§çš„ï¼ˆ83%ï¼‰ã€‚

![image-20240529132909923](.\assets\image-20240529132909923.png)



### 5.3 å¯¹transformerçš„åŠ é€Ÿ

ç»è¿‡ç¬”è€…çš„åˆ†æï¼Œtransformerçš„æ—¶é—´å¤æ‚åº¦å¤§çº¦æ˜¯

$token-length*N^2*å±‚æ•°$

è¿™ç§å¤æ‚åº¦åœ¨è¾“å…¥é•¿åº¦å˜é•¿çš„æ—¶å€™å°†é¢ä¸´å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚å®é™…ä¸Šè¿™ç§åœºæ™¯æ˜¯å¾ˆå¸¸è§çš„ï¼Œæ¯”å¦‚å¤§è¯­è¨€æ¨¡å‹å¯èƒ½éœ€è¦é˜…è¯»å¾ˆé•¿çš„æ–‡æœ¬å¹¶ç»™å‡ºæ€»ç»“ï¼Œå†æ¯”å¦‚vitä¸­ä¸€ä¸ªå›¾ç‰‡è¢«åˆ†æˆäº†éå¸¸å¤šçš„å°tokenã€‚

ä¸ºäº†å‡å°‘tokenæ¥åŠ é€Ÿï¼Œæœ‰ä¸¤ç§å¸¸è§çš„æ€è·¯ï¼Œåˆ†åˆ«æ˜¯åˆ é™¤ä¸åˆå¹¶

##### 1.token_prune(å‰ªæ)

> åœ¨å®è·µä¸­ï¼ŒTransformer æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬çš„æ¨ç†æˆæœ¬åŒ…æ‹¬å†…å­˜å ç”¨ã€å»¶è¿Ÿå’ŒåŠŸè€—ï¼Œè¿™äº›æˆæœ¬ä¸è¾“å…¥åºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºå­¦ä¹ ä»¤ç‰Œä¿®å‰ª ï¼ˆLTPï¼‰ çš„æ–°å‹ä»¤ç‰Œç¼©å‡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è¾“å…¥åºåˆ—é€šè¿‡è½¬æ¢å™¨å±‚æ—¶è‡ªé€‚åº”åœ°åˆ é™¤ä¸é‡è¦çš„ä»¤ç‰Œã€‚å…·ä½“è€Œè¨€ï¼ŒLTP ä¼šä¿®å‰ªæ³¨æ„åŠ›åˆ†æ•°ä½äºé˜ˆå€¼çš„ä»¤ç‰Œï¼Œåœ¨è®­ç»ƒæœŸé—´ä¼šå­¦ä¹ æ¯ä¸€å±‚çš„å€¼ã€‚

æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå¦‚æœè¿™ä¸ªtokenæ²¡æœ‰ä»»ä½•äººå»æ³¨æ„ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸é‡è¦çš„ã€‚

##### 2.token merging(åˆå¹¶)

> æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆå¹¶å†—ä½™æ ‡è®°æ¥åˆ©ç”¨ç”Ÿæˆå›¾åƒä¸­çš„è‡ªç„¶å†—ä½™æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚åœ¨å¯¹ Token Merging ï¼ˆToMeï¼‰ è¿›è¡Œäº†ä¸€äº›ç‰¹å®šäºæ‰©æ•£çš„æ”¹è¿›åï¼Œæˆ‘ä»¬çš„ ToMe for Stable Diffusion å¯ä»¥å°†ç°æœ‰ Stable Diffusion æ¨¡å‹ä¸­çš„tokenæ•°é‡å‡å°‘å¤šè¾¾ 60%ï¼ŒåŒæ—¶æ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒå³å¯ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å›¾åƒç”Ÿæˆé€Ÿåº¦æé«˜äº† 2Ã—å¹¶å°†å†…å­˜æ¶ˆè€—é™ä½äº† 5.6Ã—ã€‚

token mergingå®é™…ä¸Šå°±æ˜¯æŠŠæ¯”è¾ƒç±»ä¼¼çš„tokenåˆå¹¶äº†ã€‚ä»è€Œå‡å°‘äº†æ€»çš„tokenæ•°ç›®ã€‚

![image-20240529123959845](.\assets\image-20240529123959845.png)

##### 3.å®éªŒ

ç¬”è€…é€‰å–äº†token mergingçš„ä»£ç å¹¶åœ¨æˆ‘ä»¬çš„å®éªŒç¯å¢ƒä¸‹è¿›è¡Œäº†å®éªŒã€‚åœ¨è¿™é‡Œæˆ‘ä»¬åªè¿›è¡Œç®€å•çš„æ¼”ç¤ºã€‚

![image-20240610225257823](.\assets\image-20240610225257823.png)

è¿™é‡Œè®©æ¨¡å‹ç”Ÿæˆä¸€å¼ student of university of science and technology of chinaçš„ç…§ç‰‡ã€‚

>  tomesd.apply_patch(model, ratio=0.5)

é˜…è¯»è®ºæ–‡å¾—çŸ¥ï¼Œè¿™ä¸ªè®¾ç½®å‡å°‘äº†ä¸­é—´å±‚25%çš„tokensã€‚è¿™æ˜¯å› ä¸ºå›¾ç‰‡çš„å†—ä½™ä¿¡æ¯æ¯”è¾ƒå¤šï¼Œå¦‚æœæ˜¯è¯­è¨€æ¨¡å‹å¯èƒ½ä¸èƒ½åˆ æ‰è¿™ä¹ˆå¤šã€‚

ä¼˜åŒ–å‰æ‰§è¡Œ31s

![image-20240610225727708](.\assets\image-20240610225727708.png)

ä¼˜åŒ–åæ‰§è¡Œ25s

![image-20240610225647278](.\assets\image-20240610225647278.png)

å›¾ç‰‡æ•ˆæœä¾ç„¶éå¸¸å¥½ï¼Œå·¦è¾¹ä¸ºä¼˜åŒ–å‰ï¼Œå³è¾¹ä¸ºä¼˜åŒ–åã€‚

![image-20240610225919546](.\assets\image-20240610225919546.png)

## ä¸ƒã€Conclusionï¼ˆæ€»ç»“ï¼‰

åœ¨æœ¬æ¬¡è°ƒç ”è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…æ·±å…¥é˜…è¯»äº†ã€ŠAttention is All You Needã€‹è¿™ç¯‡è®ºæ–‡çš„æ¯ä¸€ä¸ªéƒ¨åˆ†ï¼Œè¿˜ç»“åˆä»£ç å®ç°è¯¦ç»†äº†è§£äº†Transformerçš„å®ç°ç»†èŠ‚ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†å“ˆä½›å¤§å­¦å¼€æºçš„å®ç°â€”â€”"The Annotated Transformer"ï¼Œä»¥åŠä¸€ä¸ªé€‚ç”¨äºå°è§„æ¨¡æ•°æ®é›†çš„multi30kæ•°æ®é›†ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥åœ¨15minå·¦å³çš„æ—¶é—´å°±å®Œæˆä¸€æ¬¡è®­ç»ƒï¼Œè¿™å¤§å¤§æé«˜äº†æˆ‘ä»¬çš„æ¢ç´¢æ•ˆç‡ã€‚

åœ¨è°ƒç ”è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®åˆ†å·¥ï¼Œæ¯ä½ç»„å‘˜éƒ½é«˜æ•ˆåœ°è´¡çŒ®äº†è‡ªå·±çš„åŠ›é‡ï¼Œä½¿å¾—æœ€ç»ˆçš„æŠ¥å‘Šå†…å®¹éå¸¸ä¸°å¯Œã€‚æˆ‘ä»¬å¯¹Transformerçš„åŠ¨æœºã€å®ç°å’Œæ‰©å±•éƒ½è¿›è¡Œäº†å…¨é¢è€Œè¯¦ç»†çš„æè¿°ã€‚

Transformerä½œä¸ºä¸€ç§é‡è¦çš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œåœ¨æœ¬æ¬¡è°ƒç ”ä¸­è·å¾—äº†ç‰¹æ®Šçš„å…³æ³¨ã€‚ç›¸ä¿¡è¿™æ¬¡è°ƒç ”å¯¹äºæ¯ä½ç»„å‘˜éƒ½å¸¦æ¥äº†ä¸°åšçš„æ”¶è·ã€‚



