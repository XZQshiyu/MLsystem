大模型推理问题

- Decode阶段：低计算访存比（原因：LLM推理是自回归串行解码模式，单请求输入为1个token，操作为向量矩阵乘）

  解决思路：

  - 多请求组 Batch 处理
  - 单请求并行解码（如投机推理）

- 长序列 Prefill：高计算量、高内存占用

  解决思路：

  - KVCache多请求间缓存复用（如prefix caching）
  - 长序列Prefill多机并行计算/稀疏注意力

- GPU/NPU 内存容量/带宽瓶颈：

  解决思路：

  - 内存分配机制优化（如PageAttention、KVCache offload）
  - Attention算子优化（如Flash Attention）
  - 压缩量化剪枝
  - Attention机制优化（如GQA、MQA、MLA）

- 场景演进：

  - MoE
  - 多模态
  - RAG

PD分离是什么：将计算密集的Prefill和访存密集的Decode分开计算，避免同时组批互相干扰

