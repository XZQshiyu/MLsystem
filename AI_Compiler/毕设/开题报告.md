# 基于昇腾NPU的归约类算子设计与优化

| 中文名称 | 基于昇腾NPU的归约类算子设计与优化                        |
| -------- | -------------------------------------------------------- |
| 英文名称 | Design and Optimization of Operators Based on Ascend NPU |

## 课题简介

### 1.背景

#### 1.1 基于 NPU 的深度学习编译器带来的问题和挑战

随着大模型的迭代更新和各类 NPU 芯片的问题层出不穷，基于人工优化的方式给算子开发团队带来了沉重的负担。因此，开发一种能够将高层次算子表示编译成目标硬件可执行代码的算子编译器[^1]，逐渐成为学术界及工业界的共识。算子编译器通过基于模版（手动）、搜索算法或优化求解算法（自动）等方式完成循环变换、循环切分等调度优化，以及硬件指令映射、内存分配、指令流水等硬件相关优化。

但是，现有算子编译器的中间表达在兼容上层应用和 NPU 硬件指令时存在兼容问题。NPU通常提供了更多的指令选择、多级的存储结构以及复杂的数据流处理，提升了循环优化、内存优化、并行优化以及指令优化的复杂度，降低了 NPU 算子在深度学习框架（诸如 Pytorch[^2]，Triton[^3]）算子上的覆盖率，导致在缺乏人工适配的应用上 NPU 普遍存在不能用、不好用的问题。

- 访存带宽低：NPU缓存间的数据搬移性能依赖于片上 DMA 的实现，处理器离散访存能力较弱，弱化了存取操作的表达能力，不适合 Element-Wise 等类型的张量操作。此外，多级缓存亟需透明地访存抽象。
- 缓存空间小：为了减小延迟，NPU 通常会使用多级缓存（L0/L1/L2/UB）来减少数据访问时间，并使用乱序执行来隐藏数据等待时间。但是片内缓存大小有限，很难保持内部缓存的统一抽象，进一步增加了内存分配和访存延迟隐藏方案优化的复杂度。
- 同步开销大：NPU需要进行细粒度的同步来获得高效代码，但是同步指令的准确插入对性能影响极大。因此，根据循环相关信息分析数据依赖关系，不仅要考虑了合适的插入位置，还要考虑搬移数据量的大小是否满足程序的合法性约束
- 并行门槛高：NPU缺乏循环机自动流水并行优化，以及自动完成向量/张量指令映射的机制，导致多核和单算子多核并行度不匹配，需要插入张量/向量核的数据通信和同步指令。

### 2.问题描述

#### 1.2 算子自动生成

目前多个开源深度学习框架/编译器已经支持自动生成算子代码。许多编译器的设计灵感来自 Halide[^4] ，包括 TVM[^5]、nvFuser[^6]、NNC[^7]。这些设计有独立的语义语言和调度语言，可以在不改变程序语义的情况下尝试不同的调度。MLIR[^8]生态系统中出现了很多更新的编译器，包括 IREE[^9]。Triton[^10]也使用 MLIR 作为其内部表示方法。使用 Triton能够比手写库 [^11]更快地生成内核，而且输入代码也很简单。很少有编译器能够始终如一地做到这一点，许多只是直接调用这些库而不试图在生成复杂内核方面改进。

本研究拟针对 NPU 中间表达同上层和底层硬件接口不兼容的问题，为 NPU 中差异性和多样性的张量表示和张量计算核设计中间表达和调度抽象，设计适配上层应用的简单高效的编程接口，覆盖不同的硬件后端，自动生成大模型的关键算子和常规算子，解决相同算子在不同硬件上的重复开发问题，实现对新硬件的快速支持。需要解决的关键问题包括：

-  算子种类繁多，特点各异
- 理解并丰富现有算子中间表达
- 实现算子代码模版

#### 1.3 归约类算子

在如 Triton 编译器中，归约类算子是一种重要的工具，用于高效执行诸如求和、乘积、最大值和最小值等归约操作。这类算子在深度学习和科学计算中非常普遍，例如在矩阵操作中的逐元素求和、卷积操作中的最大池化，以及其他对张量数据进行压缩的任务中都需要用到归约操作。Triton 的归约算子通过 tile-based 并行方法优化了这些计算的执行效率，从而能够在 GPU 上实现高效优化，但对于 NPU 等架构上的优化效果并不明显。

本课题旨在研究如何使得能够在 昇腾NPU上生成高效可用的归约类算子，并以 triton language中的 sum算子为例，探讨其对 昇腾 NPU 系列芯片的硬件适配和算子的高效编译实现。

### 2.问题描述

## 3.设计与实现

4.



#### 1.1 深度学习编译器的重要地位和工作流程

随着大模型技术的不断演进，深度学习编译器既要对上承接模型算法的优化，满足算法开发者不断探索的研究需求，又要对下在最终的二进制输出上满足多样性硬件的诉求，满足不同部署环境的资源要求。深度学习编译器的设计收到了主流编译器（如LLVM[^4]）的影响，一般采用多层级中间表示设计。前端中间表示是通过解析用户代码生成的，属于一个较高的抽象层次，隐藏了一些底层运行的细节信息。通过对前端中间表示图上的各个节点进行拆分和融合，可以逐步转换为更加贴合硬件的中间表示。此时，对于单节点的中间表示可能仍然有很多种不同的选择，需要针对中间表示图进行细粒度优化。例如，可以选择不同的输入输出格式和数据类型。最终，在生成完整的算子序列后，需要为每个算子分配相应的输入输出内存。

#### 1.2 扩展到深度学习加速器的原因和意义

当前主流大模型大多基于神经网络实现（如 Transformer[^5]），无论是训练还是推理，都会产生海量的计算任务，尤其是涉及矩阵乘，softmax 中的归约类算子这种高计算任务的算子。然而，通用处理器芯片诸如 CPU 在执行这类算子时通常耗时比较大，难以满足训练和推理任务的需求。因此工业界和学术界都将目光投向特定领域的加速器芯片人机，希望以此来解决算力不足的问题。

对于不同的加速器设计方向，业界也有不同的硬件实现。针对架构的通用性，NVIDIA持续在 GPU 芯片上发力，先后推出了 Volta、Turing、Ampere等架构 [^6]，并推出用于加速矩阵计算的张量计算核心，以满足深度学习海量算力的需求。对于偏定制化的硬件架构，面向深度学习计算任务，业界提出了神经网络加速器（NPU）。华为推出了昇腾NPU[^7] ，旨为用户提供更高能效的算力和易用的开发、部署体验。昇腾 NPU 使用 CUBE 运算单元（类 SIMD）来加速矩阵乘法的计算。

#### 1.3 基于 NPU 的深度学习编译器带来的问题和挑战

随着大模型的迭代更新及各类 NPU 芯片的层出不穷，基于人工优化算子的方式给算子开发团队带来沉重的负担。因此

### Triton简介

Triton[^1]是一种专为深度学习优化的编译器，主要用于提升 GPU 上的计算效率，尤其适用于深度学习中广泛使用的张量运算。其开发旨在帮助如 PyTorch 等框架实现更高效的 GPU 核心计算。Triton 采用了基于 tile（分块）的计算方式，将大规模的计算任务分解为更小的、易于管理的块，并通过 GPU 的并行能力加速处理。这种设计允许 Triton 生成高性能的 GPU 内核，性能上甚至优于一些手动优化的库，如 cuDNN[^2] [^3]

Triton 的优势还在于它对动态计算图的支持，使其能够高效地处理神经网络中的复杂操作，这对于 PyTorch 等动态框架尤为重要。Triton 的模块化设计不仅简化了 GPU 编程的复杂性，还为深度学习任务提供了显著的加速效果。研究表明，Triton 能够在不深入了解 CUDA 编程的情况下实现与 NVIDIA 张量核心相媲美的性能，提供了高效的 GPU 加速替代方案。

### MLIR 简介



### 归约类算子设计与优化



[^1]:Philippe Tillet, H. T. Kung, and David Cox. 2019. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages (MAPL 2019). Association for Computing Machinery, New York, NY, USA, 10–19. https://doi.org/10.1145/3315508.3329973
[^2]: Su, Q., Geng, C., Pekhimenko, G., & Si, X. (2023). TorchProbe: Fuzzing Dynamic Deep Learning Compilers. arXiv preprint arXiv:2310.20078. https://arxiv.org/abs/2310.20078
[^3]: Katel, N., Khandelwal, V., & Bondhugula, U. (2021). High Performance GPU Code Generation for Matrix-Matrix Multiplication using MLIR: Some Early Results. *arXiv preprint arXiv:2108.13191*. https://arxiv.org/abs/2108.13191
[^4]:
[^5]:
[^6]:



## 主要研究学科与方向



## 时间安排



## 总述











- 背景描述（400）

- 问题描述 (2- > 1)（800）
- 问题产生的原因
- 设计与实现
  - 打算怎么做（400）
  - 具体手段（400）
- 验证与测试